[{"title":"数据存储/MySQL/06慢查询及优化","date":"2020-03-27T03:51:48.000Z","path":"2020/03/27/数据存储/MySQL/06慢查询及优化/","text":"慢查询及优化查询的执行过程：客户端，到服务端后：会话连接、SQL分析、SQL 优化、生成执行计划、执行后返回结果。执行是整个生命周期最重要的阶段。 查询分析 EXPLAIN 属性 含义 id 查询中执行 select 子句或操作表的顺序，Id 相同执行顺序由上向下；id 越大优先级越高越先执行 select_type select 子句查询类型，主要区别普通查询、联合查询、子查询等复杂查询 table 数据库中表名称，行数据是关于哪张表，可能不一定是真实的表名称 partitions 分区表命中的分区情况，非分区表该字段为空 type 访问类型，对表的访问方式 possible_keys 使用哪个索引能找到记录即该查询可以利用的索引。查询的列上若存在索引会被列出。如果没有则显示 NULL key 实际查询过程中用到的索引，一定包含在 possible_keys 中，如果没有则显示 NULL key_len 索引使用的字节数，如果是单列索引则是整个索引长度；如果是多列索引，则具体用到多少列索引就算多少 ref 列与索引的比较，表的连接匹配条件，哪些列或常量被用于查找索引列上的值 rows 估算结果集行数 filtered 返回的结果行和需要扫描读到的行数的比值 Extra 解决查询的详细信息 select_type simple：简单的 select 查询，查询中不包含子查询或者 union primary：子查询中最外层查询，查询中包含任何复杂的子部分，最外层被标记 subquery：在 select 或 where 列表中包含子查询，结果不依赖外部查询 dependent subquery：子查询中第一个 select，依赖外部查询 uncacheable subquery：一个子查询的结果不能被缓存，必须重新评估外链接第一行 derived：在 from 列表中的子查询，MySQL 会递归执行这些子查询，然后把结果集放在临时表中 union：如果第二个 select 出现在 union 之后，则被标记为 union；如果 union 包含在 from 子句的子查询中，外层 select 被标记为 derived union result：union 的结果，union 语句中第二个 select 后面的所有的 select dependent union：union 中第二个或后面的 select 语句 type system：表中仅有一行，const 连接类型的特殊情况 const：通过索引一次就找到，const 用于比较主键索引和唯一索引，如果将主键放在了 where 条件中，MySQL 可以将其转换为常量 eq_ref：唯一性索引扫描，对于每个索引键表中只会有一行记录与之匹配 ref：非唯一性索引扫描，返回匹配某个单独值的所有行，本质上也是一种索引访问，它返回所有匹配某个单独值的行，可能会找多个符合条件的行，属于查找和扫描的混合体 ref_or_null：与 ref 类似，增加了 null 值比较 range：使用索引的范围扫描，见于使用 =, &lt;&gt;, &gt;, &gt;=, &lt;, &lt;=, IS NULL, &lt;=&gt;, BETWEEN, IN()或者like等运算符的查询中 index：遍历索引树，不需要读取数据行，例如只通过覆盖索引就查找到目标数据 all：全表扫描，然后在服务层根据条件过滤返回需要的记录 fulltext：全文索引，优先级较高，如果全文索引和普通索引同时存在，MySQL 会优先使用全文索引 index_merge：查询使用了两个以上的索引，最后取交集或者并集 unique_subquery：用于 where 中的 in 形式子查询，子查询返回不重复唯一值 index_subquery：用于 in 形式的子查询使用到了辅助索引或常数列表，子查询可能返回重复值，可以使用索引将子查询去重 ref常数等值查询显示 const，连接表查询显示驱动表的关联字段，使用了表达式、函数、条件列发生内部隐式转换显示 func Extra distinct：在 select 使用了 distinct 关键字 no table used：不带 from 子句的查询或者 from dual 使用 not in() 形式子查询或者 not exist 连接查询，这种叫做反连接。一般连接查询是先查内表再查外表，这种是先查外表再查内表 using filesort：排序时无法使用到索引，常见于 ORDER BY 和 GROUP BY 语句中 using index：查询时不需要回表，直接通过索引就可以查询 using where：存储引擎返回的记录不满足查询条件，需要在服务层通过条件过滤 using join buffer：使用了连接缓存，减少内表的循环数量以及顺序的扫描查询 using sort_union：表示使用 and 的各个索引的条件时，该信息表示是从处理结果获取交集 using_union：表示使用 or 连接各个使用索引的条件时，该信息表示从处理结果获取并集 using sort_union：表示使用 and 连接查询信息量大时，先查询主键，然后进行排序合并后，才能读取记录并返回。 using sort_intersection：表示使用 or 连接查询信息量大时，先查询主键，然后进行排序合并后，才能读取记录并返回。 firstmatch(td_name)：常见于 where 子句还有 in() 类查询，内表数据量大可能出现 loosescan(m…n)：在 in() 类型的子查询中，子查询返回的可能有重复记录 慢查询查询性能最基本的原因是访问不必要或者太多的数据。主要通过两个步骤来分析：应用是否检索了大量超过实际需要的数据，包括太多的行数据或者列数据；服务层是否在分析大量超过实际需要的行。 不需要的数据 查询不需要的记录：利用 limit 优化自己需要的记录行 多表关联时返回全部列：注意多表关联的情况，一般可能只需要关联表的某个字段而不是全部字段 取出全部列：使用了 SELECT *，可能情况比较少。使用 * 会让优化器无法利用覆盖索引完成优化，取出实际需要的列即可 重复查询相同的数据：执行相同的查询、返回相同的数据。考虑合理利用缓存 扫描额外的记录主要是在查询执行过程中是否扫描了过多的行数据，考虑是否可以优化 响应时间：响应时间主要包括两部分：服务时间和排队时间，服务时间是指数据库处理这个查询花费的具体时间，排队时间是服务器要等待某些资源没有没有执行查询的时间 扫描的行数和返回的行数：最好的情况就是扫描的行就是需要的返回的行，避免无用的行扫描 扫描的行数和访问类型：EXPLAIN 的 type 列反映了访问类型(全表扫描、索引扫描、范围扫描、唯一索引扫描、常数引用)。所以可以让 MySQL 以高效、扫描行数少的方式找到需要的记录 MySQL 应用 WHERE 条件的场景： 在索引中使用 WHERE 条件在存储引擎层过滤不匹配的记录 使用索引覆盖扫描来返回记录，在 EXTRA (using index) 。直接从索引中返回结果这是在服务器层完成不需要回表查询 从数据表中返回结果，过滤不满足条件的记录，在 EXTRA (using where) 。在服务层完成，先从数据表读出记录然后过滤 如果发现需要扫描大量的行但是只返回少量的数据，可以从下面几个方面考虑优化： 是否可以优化为从覆盖索引扫描，把需要的列放在覆盖索引中，查询索引时直接返回 优化数据库、表结构 优化查询 SQL","tags":[]},{"title":"数据存储/MySQL/05高性能索引","date":"2020-03-27T02:13:08.000Z","path":"2020/03/27/数据存储/MySQL/05高性能索引/","text":"索引方便存储引擎快速查找，索引可以包含一列或多列。 索引的类型B-Tree 索引即是 BTree，在不同的存储引擎底层可会有不同的实现，NDB 存储引擎内部使用 T-Tree 结构存储；InnoDB 使用的是 B+Tree。 BTree对索引列是顺序存储的， BTree可以加快访问数据的速度，使存储引擎不在需要全表扫描，从索引的根节点开始搜索。 可以使用 BTree 索引的查询类型123456789create table index_test( aa varchar(10) not null, bb varchar(10) not null, cc varchar(10) not null, dd varchar(10) not null, key(aa, bb, cc));insert into index_test(aa, bb, cc, dd) values(&quot;a1&quot;, &quot;b1&quot;, &quot;c1&quot;, &quot;d1&quot;),(&quot;a2&quot;, &quot;b2&quot;, &quot;c2&quot;, &quot;d2&quot;),(&quot;a3&quot;, &quot;b3&quot;, &quot;c3&quot;, &quot;d3&quot;); 全值匹配：和索引中的所有列进行匹配；上述索引可以用于查找 aa=a1,bb=b1,cc=c1 的行 匹配最左前缀：即组合索引中使用索引的第一列；可以查找 aa=a1 的行 匹配列前缀：匹配某一列值的开头部分；可以查找 aa 列以 a 开头的行 匹配范围值：查找 aa 在 a1 和 a3 之间的行 精确匹配某一列并范围匹配另外一列：可以查找 aa=a1 AND bb=&#39;%b&#39; 的行 只访问索引的查询： BTree索引限制 如果不是按照索引的最左列开始查找则无法使用索引；无法使用索引查找 cc=c3 的行 不能跳过索引的列；例如 aa=a1 AND cc=c1 则只使用了第一列索引，第三列索引未使用 查询中有某个列的范围查询则其右边的列都无法使用索引查询，例如 WHERE aa=&quot;a1&quot; AND BB = &quot;%1&quot; AND cc=&quot;c1&quot; 中 CC 这里只能使用索引的前两列 在实际的业务项目中给那些列加索引需要重点分析，在编码过程中 WHERE 列的匹配顺序也很重要。 哈希索引基于哈希表实现，只有精确匹配索引所有列的查询才有效，存储引擎会给每一行的索引列计算一个哈希码，不同键值的行计算出的哈希码不一样，哈希索引将所有的哈希吗存储在索引中并在哈希表中保存了索引指向每一行数据的指针。如果哈希码计算产生了哈希冲突，哈希索引会以链表的方式存储在一个哈希条目中。 在具体的查询中，先计算查询条件的哈希值，然后在哈希索引中查找对应索引然后根据数据指针找到对应的数据行比对数据是否是要查询的数据 哈希索引的限制 哈希索引中只存索引值和行数据指针，所以不能根据索引直接比对数据，需要定位到对应的行 哈希索引并不是按照索引值排序存储的，也就无法用于排序 哈希索引不支持部分索引列匹配查找，哈希索引是根据索引列的全部内容来计算索引值的，在hash(aa, bb)上建立哈希索引，如果只是用 aa列查找则无法使用索引 由于哈希索引的计算和存储格式，只能支持等值(= IN &lt;=&gt;)查找不支持范围查找 哈希冲突较少时，哈希索引的访问速度非常快；哈希冲突多的时候会造成索引维护成本变高，也会造成查询效率变低 InnoDB 的自适应性哈希索引会在某个索引被频繁使用时，在 B-Tree 索引之上在建立一个哈希索引。 空间数据索引(R-Tree)MyISAM 支持空间索引，可以做地理数据存储。可以从所有维度来索引数据，可以从任意维度组合索引查询。但是必须使用 MySQL 的 GIS 相关函数 MBRCONTAINS() 来维护数据，在这个方面 PostgreSQL 支持的较好。 全文索引适合查找文本中的关键字，而不是比较索引中的值。全文索引更适合做搜索引擎。 聚簇索引将数据与索引放在一起存储，避免数据冗余存储所以一个表只有一个聚簇索引。聚簇索引默认使用主键作为 key，没有主键使用唯一键，还没有的话使用 6 字节的 rowId。 索引的优点 大大减少服务需要扫描的数据量 索引可以帮助服务器避免排序和建立临时表 将随机 IO 变为顺序 IO 索引为查询提供了很好的性能，但是当表的数据量特别大时，维护索引的代价也会增加。 索引策略独立的列查询条件中的列需要是独立的，列不能是表达式的一部分，也不能是函数的参数 前缀索引和索引选择性索引的选择性是指不重复的索引值和数据表的记录总数(T)的比值，范围是从 1/T ~ 1 唯一索引的选择性是最好的性能也是最好的。 对于一些数据类型必须要使用前缀索引(BLOB、TEXT、过长的VARCHAR)，索引这些列的完整长度，资源耗费太大。前缀索引的长度也是需要根据具体的数据分析。前缀索引是的索引更小更快但是 MySQL 无法使用前缀索引做 GROUP BY、ORDER BY 和覆盖扫描。 组合索引创建索引的时候并不是给每个列创建索引，可以结合具体的业务查询，可以创建多列索引。尽可能是的索引的选择性更高。 当服务器对多个索引做相交操作(多个 AND 条件)时，这时建立包含相关列的组合索引效果会更好 当服务器对多个索引做相交操作(多个 OR 条件)时，通常会耗费大量 CPU、内存资源在缓存、排序和合并操作上 索引顺序在一个多列索引中，索引列顺序按照最左匹配开始，所以索引列的顺序很重要。一般情况下将选择性最高的索引列放在最左边，这样可以降低后面索引列的查找范围。这个在 B-Tree 索引中有介绍。 聚簇索引聚簇索引并不是一种索引类型，而是一种数据存储方式。索引依赖与具体的存储引擎，并不是所有的存储引擎都实现了聚簇索引，InnoDB 的聚簇索引叶子节点保存了 key 和数据行，非叶子只保存 key。因为保存了数据行，所以一个表只能有一个聚簇索引。InnoDB 将主键作为聚簇索引的 key，如果没有主键使用唯一键，还没有的话会定义一个隐式的主键来作为聚簇索引。 优点： 索引和数据保存在一起，找到索引即找到数据，数据访问更快 使用覆盖索引扫描的查询可以直接使用叶节点的主键值 缺点： 对于 I/O 密集的应用聚簇索引可以提高查询的性能，但是如果数据都放在内存中时这种优势就下降了 聚簇索引的叶子节点是有序的，当插入新的数据时，插入速度依赖于主键顺序。 当要更新聚簇索引的索引列，这样可能引起底层叶子节点的数据迁移，这样会影响插入性能 当插入数据和更新索引列，有可能会引起“页分裂”(新插入的行和索引变化列需要放到某个已满的页)，这个时候需要将已满页的数据调整，叶子节点要调整上层非叶子节点也可能要调整。 聚簇索引可能会导致全表扫描变慢。尤其是行比较稀疏的情况，这是因为数据行分布在不同的页上，需要将数据加载到内存，此时 I/O 的损耗较大。 建立了聚簇索引，会导致二级索引(非聚簇索引)访问需要两次查找，先在二级索引查到对应的主键列然后在聚簇索引中查找数据。前面说的 InnoDB 引擎自适应性哈希可以减少这样的工作。二级索引存储主键列不是行指针，这样可以减少页分裂和行移动时的二级索引维护工作 可以看到建立聚簇索引主键的性质对于聚簇索引很重要，如果主键是有序自增长的则在插入数据时会更方便一点。如果是随机的例如 UUID，则会存在几个缺点： 插入的数据不一定是在最后可能是已有数据的中间，这样可能导致 InnoDB 不得不频繁的做页分裂，以便为新的行数据分配空间 数据航要写入的页可能已经刷到磁盘上并从缓存中移除，或者还没有被加载到缓存中，此时要插入数据时需要先从磁盘读取页数据，增大了 I/O 开销 频繁的页分裂，页会变的稀疏并被不规则的填充最终有数据碎片产生 当然顺序的主键因为要控制自增，在并发情况下可能会导致间隙锁竞争和 AUTO_INCREMENT锁竞争 覆盖索引索引的叶子节点已经包含(覆盖)了要查找的字段，不需要在回表查询。所以索引在设计上要尽可能多考虑到后续的查询场景。覆盖索引对于性能的提升很有帮助。 覆盖索引必须要存储索引列的值，MySQL 只能使用 B-Tree 做覆盖索引 索引条目远小于数据行，所以如果只读取索引，可以减少缓存的负载、I/O 的消耗 索引按照列值顺序存储，对于 I/O 密集型的范围查询比随机读取每一行数据的 I/O 要少 MyISAM 在内存中缓存索引，访问数据时需要访问磁盘进行一次系统调用。如果在内存中可以直接获取性能会更好 二级索引存在查询聚簇索引的情况，如果二级索引能够覆盖查询则可以避免对主键索引的二次查询 使用索引做排序扫描索引本身是很快的，但是索引要是没有完全包含要查询的列，则需要在查询的时候回表查询对应的数据行，这样的话反而比顺序全表扫描慢了。 当索引的列顺序和 ORDER BY 的列顺序完全一致，并且所有列的排序方向 都一样时才能使用索引排序；如果查询关联多张表只有当 ORDER BY 子句引用的字段全部是第一个表的才能使用索引排序，ORDER BY 也要求满足索引的最左前缀匹配原则。当前导列为常量时， ORDER BY 不满足最左前缀要求也可以用索引排序。 压缩(前缀压缩)索引MyISAM 使用前缀压缩来减少索引的大小，让内存可以存放更多的索引。MyISAM 压缩每个索引块的方法是：保存索引块中的第一个值，然后把后面的索引对比得到相同的前缀部分和不同的后缀部分，把这部分存储起来即可。例如：第一个索引是 abcd，第二个是 abcdefg，第二个所以压缩存储后是 4,efg。MyISAM 对行指针也采取类似存储做法。 压缩块减少了存储空间但是在查询时每个索引值计算依赖前面的值，使得查找时无法使用二分查找只能从头开始扫描。 冗余和重复索引重复索引是指在相同的列上按照相同的顺序创建相同类型的索引，例如给主键再加索引 冗余索引是指要创建的索引已经可以被包含在其他的索引中，例如已经有 KEY(a, b)，再创建 KEY(a)，此时 (a) 是 (a, b) 的前缀索引。所以创建索引的时候应该尽量在之前的基础上进行扩展而不是一味新建索引。但是扩展时也得考虑扩展后会不会导致索引的维护成本变得更高。 删除未使用的索引在前期架构设计时可能存在缺陷，存在一些无用的索引，可以利用 INFORMATION_SCHEMA.INDEX_STATISTICS 查看使用频率低的索引，将其删除。 索引和锁索引可以在查询的时候锁定更少的行，减少 InnoDB 访问的行数，InnoDB 在二级索引上使用共享(读)锁，但访问主键索引需要排他(写)锁。","tags":[]},{"title":"数据存储/MySQL/mysql执行计划","date":"2020-03-24T12:49:05.954Z","path":"2020/03/24/数据存储/MySQL/mysql执行计划/","text":"mysql执行计划​ 在企业的应用场景中，为了知道优化SQL语句的执行，需要查看SQL语句的具体执行过程，以加快SQL语句的执行效率。 ​ 可以使用explain+SQL语句来模拟优化器执行SQL查询语句，从而知道mysql是如何处理sql语句的。 ​ 官网地址： https://dev.mysql.com/doc/refman/5.5/en/explain-output.html 1、执行计划中包含的信息 Column Meaning id The SELECT identifier select_type The SELECT type table The table for the output row partitions The matching partitions type The join type possible_keys The possible indexes to choose key The index actually chosen key_len The length of the chosen key ref The columns compared to the index rows Estimate of rows to be examined filtered Percentage of rows filtered by table condition extra Additional information id select查询的序列号，包含一组数字，表示查询中执行select子句或者操作表的顺序 id号分为三种情况： ​ 1、如果id相同，那么执行顺序从上到下 1explain select * from emp e join dept d on e.deptno = d.deptno join salgrade sg on e.sal between sg.losal and sg.hisal; ​ 2、如果id不同，如果是子查询，id的序号会递增，id值越大优先级越高，越先被执行 1explain select * from emp e where e.deptno in (select d.deptno from dept d where d.dname = 'SALES'); ​ 3、id相同和不同的，同时存在：相同的可以认为是一组，从上往下顺序执行，在所有组中，id值越大，优先级越高，越先执行 1explain select * from emp e join dept d on e.deptno = d.deptno join salgrade sg on e.sal between sg.losal and sg.hisal where e.deptno in (select d.deptno from dept d where d.dname = 'SALES'); select_type 主要用来分辨查询的类型，是普通查询还是联合查询还是子查询 select_type Value Meaning SIMPLE Simple SELECT (not using UNION or subqueries) PRIMARY Outermost SELECT UNION Second or later SELECT statement in a UNION DEPENDENT UNION Second or later SELECT statement in a UNION, dependent on outer query UNION RESULT Result of a UNION. SUBQUERY First SELECT in subquery DEPENDENT SUBQUERY First SELECT in subquery, dependent on outer query DERIVED Derived table UNCACHEABLE SUBQUERY A subquery for which the result cannot be cached and must be re-evaluated for each row of the outer query UNCACHEABLE UNION The second or later select in a UNION that belongs to an uncacheable subquery (see UNCACHEABLE SUBQUERY) 12345678910111213141516171819202122232425262728--sample:简单的查询，不包含子查询和unionexplain select * from emp;--primary:查询中若包含任何复杂的子查询，最外层查询则被标记为Primaryexplain select staname,ename supname from (select ename staname,mgr from emp) t join emp on t.mgr=emp.empno ;--union:若第二个select出现在union之后，则被标记为unionexplain select * from emp where deptno = 10 union select * from emp where sal &gt;2000;--dependent union:跟union类似，此处的depentent表示union或union all联合而成的结果会受外部表影响explain select * from emp e where e.empno in ( select empno from emp where deptno = 10 union select empno from emp where sal &gt;2000)--union result:从union表获取结果的selectexplain select * from emp where deptno = 10 union select * from emp where sal &gt;2000;--subquery:在select或者where列表中包含子查询explain select * from emp where sal &gt; (select avg(sal) from emp) ;--dependent subquery:subquery的子查询要受到外部表查询的影响explain select * from emp e where e.deptno in (select distinct deptno from dept);--DERIVED: from子句中出现的子查询，也叫做派生类，explain select staname,ename supname from (select ename staname,mgr from emp) t join emp on t.mgr=emp.empno ;--UNCACHEABLE SUBQUERY：表示使用子查询的结果不能被缓存 explain select * from emp where empno = (select empno from emp where deptno=@@sort_buffer_size); --uncacheable union:表示union的查询结果不能被缓存：sql语句未验证 table 对应行正在访问哪一个表，表名或者别名，可能是临时表或者union合并结果集 1、如果是具体的表名，则表明从实际的物理表中获取数据，当然也可以是表的别名 ​ 2、表名是derivedN的形式，表示使用了id为N的查询产生的衍生表 ​ 3、当有union result的时候，表名是union n1,n2等的形式，n1,n2表示参与union的id type type显示的是访问类型，访问类型表示我是以何种方式去访问我们的数据，最容易想的是全表扫描，直接暴力的遍历一张表去寻找需要的数据，效率非常低下，访问的类型有很多，效率从最好到最坏依次是： system &gt; const &gt; eq_ref &gt; ref &gt; fulltext &gt; ref_or_null &gt; index_merge &gt; unique_subquery &gt; index_subquery &gt; range &gt; index &gt; ALL 一般情况下，得保证查询至少达到range级别，最好能达到ref 12345678910111213141516171819202122232425262728293031--all:全表扫描，一般情况下出现这样的sql语句而且数据量比较大的话那么就需要进行优化。explain select * from emp;--index：全索引扫描这个比all的效率要好，主要有两种情况，一种是当前的查询时覆盖索引，即我们需要的数据在索引中就可以索取，或者是使用了索引进行排序，这样就避免数据的重排序explain select empno from emp;--range：表示利用索引查询的时候限制了范围，在指定范围内进行查询，这样避免了index的全索引扫描，适用的操作符： =, &lt;&gt;, &gt;, &gt;=, &lt;, &lt;=, IS NULL, BETWEEN, LIKE, or IN() explain select * from emp where empno between 7000 and 7500;--index_subquery：利用索引来关联子查询，不再扫描全表explain select * from emp where emp.job in (select job from t_job);--unique_subquery:该连接类型类似与index_subquery,使用的是唯一索引 explain select * from emp e where e.deptno in (select distinct deptno from dept); --index_merge：在查询过程中需要多个索引组合使用，没有模拟出来--ref_or_null：对于某个字段即需要关联条件，也需要null值的情况下，查询优化器会选择这种访问方式explain select * from emp e where e.mgr is null or e.mgr=7369;--ref：使用了非唯一性索引进行数据的查找 create index idx_3 on emp(deptno); explain select * from emp e,dept d where e.deptno =d.deptno;--eq_ref ：使用唯一性索引进行数据查找explain select * from emp,emp2 where emp.empno = emp2.empno;--const：这个表至多有一个匹配行，explain select * from emp where empno = 7369; --system：表只有一行记录（等于系统表），这是const类型的特例，平时不会出现 possible_keys ​ 显示可能应用在这张表中的索引，一个或多个，查询涉及到的字段上若存在索引，则该索引将被列出，但不一定被查询实际使用 1explain select * from emp,dept where emp.deptno = dept.deptno and emp.deptno = 10; key ​ 实际使用的索引，如果为null，则没有使用索引，查询中若使用了覆盖索引，则该索引和查询的select字段重叠。 1explain select * from emp,dept where emp.deptno = dept.deptno and emp.deptno = 10; key_len 表示索引中使用的字节数，可以通过key_len计算查询中使用的索引长度，在不损失精度的情况下长度越短越好。 1explain select * from emp,dept where emp.deptno = dept.deptno and emp.deptno = 10; ref 显示索引的哪一列被使用了，如果可能的话，是一个常数 1explain select * from emp,dept where emp.deptno = dept.deptno and emp.deptno = 10; rows 根据表的统计信息及索引使用情况，大致估算出找出所需记录需要读取的行数，此参数很重要，直接反应的sql找了多少数据，在完成目的的情况下越少越好 1explain select * from emp; extra 包含额外的信息。 12345678910111213141516--using filesort:说明mysql无法利用索引进行排序，只能利用排序算法进行排序，会消耗额外的位置explain select * from emp order by sal;--using temporary:建立临时表来保存中间结果，查询完成之后把临时表删除explain select ename,count(*) from emp where deptno = 10 group by ename;--using index:这个表示当前的查询时覆盖索引的，直接从索引中读取数据，而不用访问数据表。如果同时出现using where 表名索引被用来执行索引键值的查找，如果没有，表面索引被用来读取数据，而不是真的查找explain select deptno,count(*) from emp group by deptno limit 10;--using where:使用where进行条件过滤explain select * from t_user where id = 1;--using join buffer:使用连接缓存，情况没有模拟出来--impossible where：where语句的结果总是falseexplain select * from emp where empno = 7469;","tags":[]},{"title":"数据存储/MySQL/MySQL 内置函数","date":"2020-03-20T09:27:08.000Z","path":"2020/03/20/数据存储/MySQL/MySQL 内置函数/","text":"TIPS ON DUPLICATE KEY UPDATE使用：作用：向数据库以相同unique或者primary key插入一条记录，若已经存在该key则更新这条记录，否则则新增一条记录； 示例： 1INSERT INTO TABLE_NAME(&#39;id&#39;, &#39;column2&#39;) VALUES(&#39;&#39;, &#39;&#39;) ON DUPLICATE KEY UPDATE id &#x3D; &#39;111&#39; MySQL字符串函数： ASCII(str)： 作用：返回第一个字符串的ASCII码值；如果字符串为空则返回0； 示例： 1SELECT ASCII(&#39;2&#39;) ORD(str)： 作用：如果字符串str句首是单字节返回与ASCII()函数返回的相同值。如果是一个多字节字符,以格式返回((first byte ASCII code)256+(second byte ASCII code))[256+third byte ASCII code…] 示例： 1SELECT ORD(&#39;2&#39;)","tags":[]},{"title":"数据存储/MySQL/04范式与反范式","date":"2020-03-20T07:31:08.000Z","path":"2020/03/20/数据存储/MySQL/04范式与反范式/","text":"范式与反范式Schema 设计太多的列MySQL 存储 API 引擎在工作的时候需要在服务器层和存储引擎层之间通过行缓冲格式拷贝数据，然后在服务器层将缓冲内容解码为各个列。但是这个过程代价是比较昂贵的，列越多代价就越大。MyISAM 的定长行结构实际上与服务器层的行结构刚好匹配，所以不需要转换；但是 MyISAM 的边长行结构与 InnoDB 的行结构总是需要转换。 太多的关联关联太多会导致查询性能和并发性降低 枚举使用数据库枚举导致设计凌乱，而且枚举变化时需要执行 ALTER TABLE 这对于业务应用影响较大，尽量在上层应用中实现枚举。 合理使用 NULL 值使用 NULL 值会使列的索引和索引统计难度增加，但是也不要完全摒弃。在特殊场景中合理使用避免自定特殊值导致应用出现其他问题。 范式化优点 更新操作比反范式化要更快 当数据很好的范式化时，就是有很少或者没有重复数据 范式化的表会更小 减少数据检索的难度，可以不使用 DISTINCT 或者 GROUP BY 缺点 设计上需要关联 反范式化会造成数据冗余但是没有关联是的查询会更高效。 混用范式化与反范式化可以具体分析业务数据，将两者结合起来使用。","tags":[]},{"title":"数据存储/MySQL/03MySQL数据类型","date":"2020-03-20T03:27:46.000Z","path":"2020/03/20/数据存储/MySQL/03MySQL数据类型/","text":"MySQL 数据类型选择最优的数据类型更小的通常更好：选择满足数据范围的最小数据类型，因为他们可以占用更少的磁盘、内存和 CPU 缓存，处理时需要的 CPU 周期也更少； 简单就好：简单的数据类型处理时需要更少的 CPU 周期；例如整型比字符操作代价更低； 尽量避免 NULL 值：可为 NULL 的列使得索引、索引统计和值比较都更复杂； 整数类型整数和实数可以用来存储整数，MySQL 有 TINTINT(8)、SMALLINT(16)、MEDIUMINT(24)、INT(32)、BIGINT(64)几种类型(数字表示 N 位存储空间)，值范围是 $-2^(N-1)~2^(N-1)-1$。 整数类型也可以选用 UNSIGNED 无符号数，上述所有类型正数部分扩大为原来的 2 倍。 MySQL 可以为整数类型指定宽度，这不会影响实际的数据存储。 实数类型带有小数的数，FLOAT(32)和DOUBLE(64)类型使用标准的浮点数进行近似计算，DECIMAL用于存储精确的小数。DECIMAL只是一种存储格式，在实际计算中转换为 DOUBLE类型。MySQL 使用 DOUBLE作为内部的浮点数计算类型。 字符串类型VARCHAR、CHAR是两种主要的字符串类型。 VARCHAR可存储变长字符串，必定长的类型更节省空间。如果 MySQL 表使用了 ROW_FORMAT=FIXED 创建的话，每一行会使用定长存储，这样比较浪费空间。 VARCHAR 需要使用额外 1 或 2 个字节记录字符串长度，最大长度小于等于 255 时，使用一个字节记录，否则使用 2 字节。VARCHAR 节省了存储空间能提高性能，但是这样也导致在 UPDATE 需要更多的额外工作。例如一个行占用的空间边长，但是当前页内空间不足，MyISAM 会将行拆成不同片段存储，InnoDB 需要分裂页来存储。 适合使用 VARCHAR 的情况：字符串列最大长度比平均长度大很多；列的更新较少，碎片较少；使用了像 UTF-8 这样复杂的字符集，每个字符使用不同的字节数存储。 CHAR定长存储，CHAR 比 VARCHAR 产生更少的碎片。对于单字节的字符串更适合用 CHAR 用 VARCHAR 会占用两个字节；CHAR 在存储字符串时会将字符串末尾的空格去掉。 BINARY 和 VARBINARY 与上面的类似，他们存储的是二进制字符串，二进制字符串与常规字符串类似，但是存储的是字节码而不是字符，填充的时候是 \\0 (0字节)，而不是空格，检索时也不会去掉填充。 BLOB 和 TEXT 类型更大的字符串数据类型，分别采用二进制和字符方式存储；两者分别对应有 TINYTEXT、SMALLTEXT、TEXT、MEDIUMTEXT、LOGTEXT；TINYBLOB、SMALLBLOB、BLOB、MEDIUMBLOB、LOGBLOB。BLOB 存储二进制数据，没有字符集和排序规则， TEXT 存储字符数据有字符集和排序规则。MySQL 在对这两个数据类型排序的时候不是针对整个字符串排序，而是没个列的最前 max_sort_length 字节做排序，或者使用 ORDER BY SUSTRING(column, length)。 ENUM 类型MySQL 会将每个值在列表中的位置保存为整数，并且在 .frm 文件中保存 数字-字符串 的映射关系表。 1234create table enum_test(ee ENUM(&#39;fish&#39;, &#39;apple&#39;, &#39;dog&#39;) not null);insert into enum_test(ee) values(&#39;fish&#39;, &#39;dog&#39;, &#39;apple&#39;);select ee+0 from enum_test; # 结果： 1 3 2select ee from enum_test order by ee; # 结果：fish apple dog 可以看到在列的存储中，enum 存储的是对应位置的整数而不是具体枚举值；在排序中也是按照整数输出不是具体的字符串插入顺序。枚举的字符串列表是建表的时候初始化的，所以要修改的时候需要 ALTER TABLE。 日期和时间类型DATETIME从 1001 ~ 9999 年，精度为秒。与时区无关，占用 8 个字节存储空间。 TIMESTAMP保存了从 1970-01-01 00:00:00 以来的秒数，只使用 4 个字节存储空间。MySQL 提供 FROM_UNIXTIME 将时间戳转换为日期， UNIX_TIMESTAMP 将日期转换为时间戳。 TIMESTAMP 比 DATETIME 的空间效率更高。如果需要存储微秒级别的时间戳，可以使用 BIGINT 或者 DOUBLE 存储秒之后的小数部分，也可以使用 MariaDB。 位数据类型BITBIT 列最大长度是 64 位，MySQL 把 BIT 当作字符串类型，存储的是包含二进制 0 1 的字符串，而不是 ASCII 码值。检索 BIT 列时，例如存储的是 b&quot;00110011&quot;，在字符串环境中得到的是 ASCII 码值为 51 的字符 “9”，如果是在数字环境中则得到的是数字 51。 123create table bit_test(bb bit(8));insert into bit_test(bb) values(b&#39;00110011&#39;);select bb, bb+0 from bit_test; # 输出： 3 51 SETSET 数据类型可以利用 FIND_IN_SET、FIELD 方便查询，但是 SET 集合的修改需要使用 ALTER TABLE 进行修改；一般情况 SET 列上也无法使用索引检索。","tags":[]},{"title":"数据存储/MySQL/02MySQL性能剖析","date":"2020-03-17T12:06:20.000Z","path":"2020/03/17/数据存储/MySQL/02MySQL性能剖析/","text":"MySQL 性能剖析通常指完成某件任务所需要的时间度量，也会考虑吞吐量、CPU利用率和可扩展性等等 性能优化原则： 性能即响应时间：性能优化就是在一定的工作负载下尽可能的降低响应时间；性能优化不是降低 CPU 使用率，降低资源消耗，CPU 和资源就是被用来消耗的，如果机器不超负载的情况下，尽量利用资源提高性能。 无法测量就无法优化：要能分析出执行的时间消耗在什么地方，这样才能谈优化 性能剖析 测量任务所花费的时间 对结果进行排序，将重要的任务排在前面 理解性能剖析 值的优化的查询：性能剖析并不会自动给出哪些需要优化，具体的情况需要自己根据实际业务、耗费精力情况来分析。 异常情况：有些任务出现的情况少，但是每次出现可能会造成异常或者导致了其他情况，可能虽然不是致命的。但是要认真分析解决，因为它可能是个不定时炸弹。 未知：性能剖析的工具会显示可能的“丢失时间”，即任务的实际处理时间和测量时间不一致。需要分析是什么情况造成，应用是不是还存在未知的处理。 被掩藏的细节：性能分析的结果不能只看平均值，突刺、标准差百分比等都需要关注 查询剖析剖析服务器负载 日志文件中捕获 MySQL 查询日志，是否存在慢查询日志。慢查询日志是重要的分析依据。 利用工具将查询日志解析为报告进行分析 剖析单条查询使用 SHOW PROFILE、使用SHOW STATUS、使用慢查询日志 、使用 Performance Schema 工具针对单条查询语句做分析","tags":[]},{"title":"数据存储/MySQL/01Mysql基准测试","date":"2020-03-16T03:47:26.000Z","path":"2020/03/16/数据存储/MySQL/01Mysql基准测试/","text":"MySQL基准测试基准测试是观察系统在不同压力下的行为，评估系统的容量，掌握哪些变化是重要的，或者观察系统是如何处理不同的数据。他的主要问题就是他不是真实的压力测试，在正式的生产环境或者压力测试中，影响条件是多变的。 基准测试的策略 集成式：针对整个系统的整体测试 单组件式：单独测试 MySQL 测试指标吞吐量：单位时间内的事务处理量。TPS、TPM指标等 响应时间或延迟：测试任务所用的整体时间，计算出平均响应时间、最小响应时间、最大响应时间和所占百分比。 并发性：Web 服务器的并发性标识会话存储机制可以处理多少数据的能力，度量指标是任意时间有多少同时发生的并发请求。当并发性增加时需要关注的是吞吐量是否下降、响应时间是否延长，如果出现下降和延长的情况，那应用就有可能无法处理峰值流量。 可扩展性：系统的压力增加时，对应性能响应时间和吞吐量也应该线性增加。","tags":[]},{"title":"设计/设计模式/Head First设计模式","date":"2020-02-22T08:10:13.636Z","path":"2020/02/22/设计/设计模式/Head First设计模式/","text":"Head First设计模式OO基础 抽象、封装、多态、继承 设计原则 找出应用中可能需要变化之处，把他们独立出来，不要和那些不需要变化的代码混合在一起； 针对接口编程而不是针对实现编程 多用组合少用继承 开放关闭原则 最少知识原则：对象之间的交互要尽可能的少 策略模式 定义了算法族，分别封装起来，让他们之间可以互相替换，在这种模式下可以让算法的变化独立于他们的使用者 观察者模式 定义了对象之间的一对多依赖，这样当一个对象状态改变状态时，他的所有继承者都会收到通知并自动更新观察者模式让主题和观察者之间松耦合 装饰者模式 装饰者和被装饰者拥有共同的超类型 可以用一个或者多个装饰者对象包装对象 装饰者和被装饰对象拥有相同的超类，所以在任何需要使用超类型的地方都可以使用装饰过的对象代替 装饰者可以在被装饰者对象的行为之前或之后加上自己的行为 对象可以在任何时候被装饰 装饰者模式的装饰者的行为来自自己和组件组合，继承超类型知识为了保证 装饰者和被装饰这达到类型匹配 装饰者模式动态得将责任附加到对象上， 工厂模式 工厂模式通过让子类决定该创建什么对象来达到将创建对象的过程封装 依赖倒置 变量不可以持有具体类的引用 不要让类派生自具体类 不要覆盖基类中已经实现的方法 单例模式 确保一个类只有一个实例，并且可以提供全局访问点（类访问） 命令模式 命令模式将请求封装成一个对象，以便不同的请求、队列或日志来参数化其他对象。命令模式也支持撤销操作。 适配器模式 将一个类的接口转换成客户期望的另一个接口，适配器让原本不兼容的接口可以合作 对象适配器：通过组合实现 类适配器：通过多继承实现 外观模式 外观模式提供了一个统一的接口，用来访问系统中的一群接口。外观模式定义了一个高层接口，让子系统的功能调用更简单； 外观不只是简化了接口，也将客户从子系统的接口中解耦 外观和适配器可以包装很多类，但是外观模式知识简化了接口的使用，适配器模式是将接口转换成不同的接口； 模板方法模式 模板方法定义了一个算法的步骤，并允许子类为一个或多个步骤提供实现 模板方法可以使得子类在不改变算法结构的情况下，重新定义算法中的某些步骤 迭代器模式 提供一种方法顺序访问一个聚合对象的内部元素，而不暴露其内部的实现 组合模式 允许你将对象组合成树形结构来表现“ 整体 / 部分”层次结构，组合能让用户以一致的方式处理个别对象以及对象组合。 状态模式 状态模式允许对象在内部状态改变时改变他的行为，对象看起来好想改变了他的类。","tags":[]},{"title":"职业精神/代码整洁之道/代码整洁之道","date":"2020-02-22T08:07:46.828Z","path":"2020/02/22/职业精神/代码整洁之道/代码整洁之道/","text":"#《代码整洁之道》 勒布朗法则：稍后等于永不。 第一章 整洁代码1、糟糕代码的原因 在我自己来看，写出糟糕代码可能是以下几个原因： 编程经验不够，没有意识到代码整洁的重要性，处于功能实现即可完成任务的阶段； 有“整洁代码、优化代码”的意识，在具体编程时，由于项目进度和时间的原因，没有时间去思考如何优化，如何使代码整洁。最后只能草草了事，完成任务即可。想着可以后续继续优化，但只是停留在这想法上了。 力不从心，有优化代码的想法，但是不知道该如何进行优化、整洁。或者是不知道如何深层次的进行优化。 2、整洁代码 在Bjarne Stroustrup认为，整洁代码： 在Grady Booch看来，整洁代码为： 整洁代码2 Dave Thomas认为： 整洁代码3 Michael Feathers认为： 整洁代码4 Ron Jeffries认为： 整洁代码5 第二章 有意义的命名1、文件命名 文件一般包括类、配置文件和资源文件。文件的命名一般是名次或者名次短语。 2、方法命名 这种类型的命名一般是动词或者动词短语。方法一般是为解决某个问题或者处理某种逻辑而存在的，在命名上最好是体现出这个方法的作用 3、属性命名 普通属性可以使用驼峰式；静态变量属性最好使用A_B_C式的。命名使用英语单词，避免中英混杂，名字最好可以体现这个属性所表达的意义。 第三章 函数 函数应当尽量的短； 函数的缩进层级最多应当为2级，注意在if、else、while、for等语句块里面应当只是一个函数调用，不应再嵌套； 函数应当只做好一件事。（看一个函数是否还可以拆分） 函数名称上面也叙述过了，不要担心函数名称长度。长而具有描述性的名称是可以被接受的，命名的风格要保持一致； 函数的参数应当尽可能的少，最多为三个； 函数的书写功底也并非一日之功，想写出高效简洁就可以写出来的，是需要有一定的代码积累，有一定的编程架构思想，懂得功能分解才能达到的境界，不然也只是部分优化或者不完全优化，达不到一个高标准。 第四章 注释 以前在自己看来，多写点注视应该总是没错的，对自己看代码或者后面的开发者看总是有益无害的。但是看了本书之后，纠结于以后到底要不要写注释，且现在来看自己以前写的都是烂注释，书中对于注释要求甚是严格： 1、好注释： 提供法律信息 提供代码的有效信息 说明意图，解释代码的意图，或者是输出什么结果 可以阐释代码的结果，以进行比对，但是务必要保证注释的正确性，避免风险 有警示作用的代码，提醒读者或者此处需要注意什么问题 利用TODO或者FIX注释，标识将来需要完成或者完善的功能 2、坏注释 坏注释 第五章 代码格式第六章 对象和数据结构1、影藏对象，暴露操作 过程式代码难以添加数据结构，因为必须修改所有的函数；面向对象代码难以添加新函数，因为需要修改所有的类。 第七章 错误处理 1、避免使用返回错误码，利用异常处理错误 2、使用catch finally去捕获异常，使用不可控异常，在方法上抛出异常，会违背接口设计的开放必和原则，在它的继承和使用方法上都得抛出该异常，在底层如果修改了代码，可能会影响到其他的方法调用 3、最好在代码中给出异常可能发生的环境 第八章 边界 没看太懂 第九章 单元测试1、重视测试 在项目团队中，如果不是有要求，我想应该是没有几分人会去主动写测试类的。所以首先有这个意识很重要。项目中也有测试团队这也许导致了开发有所依赖，所以不去编写测试代码或者是随意编写测试代码。但实际上编写简洁、整洁的测试代码是对自己的代码的检验，也是一种责任的体现，团队中每个环节、每个人都能超标保证自己的任务的质量，不去依赖下一个环节，项目的质量就会提高。 2、测试的F.I.R.S.T规则 快速(Fast): 测试代码的逻辑不应太过复杂，能够快速运行，如果测试代码运行过慢，可能就不会频繁进行测试了； *独立(Independent): *每个测试方法测试类应当独立，彼此之间不能有依赖，某个测试不能成为下一个测试的依赖条件，这会导致测试很复杂； 可重复(Repeatable): 测试不应该依赖环境，在任何环境中都应当测试通过，当测试不能在任意环境中重复测试时，说明代码中存在导致其失败的接口； 自足验证(Self-Validating): 么个测试方法运行都应当有true或false输出，测试者不应当通过查询日志或主观判断进行验证。 及时(Timely): 测试应及时编写，单元测试应当在使其通过的生产代码之前编写。否则生产代码编写好之后可能会由于生产代码的逻辑复杂就不会去编写测试代码了。 第十章 类第十一章 系统 “一开始就做对系统”是不可能的，我们应当只去实现用户当前的需求，然后对系统重构，实现新的用户需求。但是在我认为，在我们有了大数据之后，我们应该去探索、预测用户的需求，一味被动等待用户有了需求在去实现就有点落后了。 第十七章 味道与启发1、注释 不恰当的信息：注释只应当描述有关代码和设计的技术性信息 废弃的注释：应当即使删除掉或者更新 冗余的注释 糟糕的注释：写注释应当和写代码一样，要思考一下 注释掉的代码：应当及时删除 2、环境 需要多少步才能实现构建：应当使用单个命令签出系统，并用单个指令构建，系统的构建不应当是分小步或者是繁琐的。 需要多少步才能做到的测试：单元测试应当可以在一个指令下全部运行。 3、函数 过多的参数：一个函数的参数应当少于3个，多余三个时应当被封装成为一个对象； 输出参数：输出单数违反直觉，可以直接改变对象的状态 表示参数：函数中的标识参数过多说明函数做了不止一件事 死函数：没有被调用的函数应当删除掉 4、一般性问题 一个源文件存在多种语言：一个源文件的语言种类应当尽可能的少，降低文件的复杂性 明显的行为未被实现：函数的实现应当按照其描述或名字定义所形容的实现 不正确的边界行为：边界行为要靠编辑测试去发现不应当靠直觉 忽视安全：警告也有可能造成程序运行的错误，对于安全机制和警告应当去处理而不是视而不见 重复：代码中又重复说明代码还有可以优化和抽象的地方 在错误的抽象层级上的代码：抽象类和派生类的界限需要根据设计划分清楚 基类依赖于派生类： 信息过多： 死代码：删除系统中存在的死代码 垂直分割：变量函数的定义和位置，应当遵循前面章节所描述的 前后不一致：同类型的变量或方法取名定义时在系统中应当是一致的 混淆视听：没有实现的构造器、没有用到的变量、没有调用的函数、没有信息量的注释；这些都应当被疑除掉。 人为耦合：声明函数、变量和常量应当有统一的地方，而不是信手拈来的位置 选择算子参数： 晦涩的意图：代码要尽可能有表达力 位置错误的权责： 不恰当的静态方法：一般情况下尽量使用非静态函数，除非该函数没必要使用多态 使用解释性变量： 函数名称应该表达其行为： 理解算法; 用多态代替if\\else和switch\\case 遵循标准约定： 用命名变量代替魔术数： 准确：消除代码的不确定性 结构基于约定： 封装条件：通常if和while语句内的判断条件如果过多都是可以封装成为一个函数的 避免否定性条件：肯定是条件要比否定式条件好理解 函数只该做一件事： 掩蔽时序耦合：当调用函数的执行是有顺序的时候，可以通过调用显示执行 别随意 封装边界条件：编辑处理的代码应当集中在一处，不应散落在代码中 函数应该只在一个抽象层级上 在较高层级放置可配置数据 避免传递浏览 6、Java 通过使用通配符避免过长的导入清单 不要继承变量 常量和枚举 7、名称 采用描述性名称 名称应与抽象层级相符 尽可能使用标准命名法 无歧义的名称 为较大作用范围选用较长的名字 避免编码 函数名称应当与函数功能相符，不应当有副作用 8、测试 测试不足：测试的覆盖率没有达到100%，就是不足 使用测试覆盖率的工具 别略过小测试 被忽略的测试就是对不确定事物的疑问 边界条件的测试 全面测试相近的缺陷 测试失败的模式有启发性 测试覆盖率的模式有启发性 测试应该快速","tags":[]},{"title":"middle-service/apm/Skywalking/Skywalking介绍","date":"2020-02-22T08:01:24.987Z","path":"2020/02/22/middle-service/apm/Skywalking/Skywalking介绍/","text":"Skywalking背景 Apache 孵化器项目 加入 OneAPM 公司 Skywalking 介绍（分布式 APM 系统 / 分布式追踪系统） 全自动探针检测，不需要修改应用程序代码（可支持的中间件和组件：） 支持手动探针监控，用埋点的方式手动上传业务数据； SkyWalking提供了支持 OpenTracing 标准的 SDK。也支持 OpenTracing-Java 支持的组件 自动监控和手动监控可以同时使用，使用手动监控弥补自动监控不支持的组件，或者私有化组件； 纯 Java 后端分析程序，提供 RESTful 服务，可为其他语言探针提供分析能力； 高性能纯流式分析； 可将 traceId。集成到主流的日志框架中输出，如 log4j，logback 等 对应的 WEB 页面由单独的工程进行发布（sky-walking-ui） Skywalking 架构图 架构图 Skywalking-web：web 可视化平台，用来展示数据 skywalking-collector：链路数据归集器，数据可以落地 ElasticSearch，单机也可以落地 H2，不推荐，H2 仅作为临时演示用 skywalking-agent：探针，用来收集和发送数据到归集器 Skywalking 目前暴露的三个问题 现象： Agent 和 Collector 正常工作，没有异常日志 已经对系统进行过访问，Trace 查询有数据 UI 除 Trace 查询页面外，其他页面无数据 原因： Collector 和被监控应用的系统主机时间，没有同步 解决方法： 同步各主机操作系统时间 现象 ： Kafka 消息消费端链路断裂 原因： Kafka 探针只是追踪了对 Kafka 的拉取动作，而整个后续处理过程不是由 kafka consumer 发起。故需要在消费处理的发起点，进行手动埋点 解决方法: 可以通过 Application Toolkit 中的@Trace标注，或者 OpenTracing API 进行手动埋点 现象： 加载探针并启动应用 Console 中被 GRPC 日志刷屏 原因：Skywalking 采用了 GRPC 框架发送数据，GRPC 框架读取 log 的配置文件进行日志输出。 解决方法: 在log的配置文件中添加对org.apache.skywalking.apm.dependencies包的过滤 Dapper 的缺点 合并的影响：我们的模型隐含的前提是不同的子系统在处理的都是来自同一个被跟踪的请求。在某些情况下，缓冲一部分请求，然后一次性操作一个请求集会更加有效。（比如，磁盘上的一次合并写入操作）。在这种情况下，一个被跟踪的请求可以看似是一个大型工作单元。此外，当有多个追踪请求被收集在一起，他们当中只有一个会用来生成那个唯一的跟踪 ID，用来给其他 span 使用，所以就无法跟踪下去了。我们正在考虑的解决方案，希望在可以识别这种情况的前提下，用尽可能少的记录来解决这个问题。 跟踪批处理负载：Dapper 的设计，主要是针对在线服务系统，最初的目标是了解一个用户请求产生的系统行为。然而，离线的密集型负载，例如符合MapReduce[10]模型的情况，也可以受益于性能挖潜。在这种情况下，我们需要把跟踪ID与一些其他的有意义的工作单元做关联，诸如输入数据中的键值（或键值的范围），或是一个 MapReduce shard。 寻找根源：Dapper 可以有效地确定系统中的哪一部分致使系统整个速度变慢，但并不一定能够找出问题的根源。例如，一个请求很慢有可能不是因为它自己的行为，而是由于队列中其他排在它前面的(queued ahead of)请求还没处理完。程序可以使用应用级的 annotation 把队列的大小或过载情况写入跟踪系统。可以采用成对的采样技术可以解决这个问题。它由两个时间重叠的采样率组成，并观察它们在整个系统中的相对延迟。 记录内核级的信息：一些内核可见的事件的详细信息有时对确定问题根源是很有用的。我们有一些工具，能够跟踪或以其他方式描述内核的执行，但是，想用通用的或是不那么突兀的方式，是很难把这些信息到捆绑到用户级别的跟踪上下文中。我们正在研究一种妥协的解决方案，我们在用户层面上把一些内核级的活动参数做快照，然后绑定他们到一个活动的 span 上。 Skywalking竞品 同类型企业级收费产品 OneAPM：http://www.oneapm.com/ 透视宝：http://www.toushibao.com/product_server.html 华为：http://www.huaweicloud.com/product/apm.html?utm_source=Baidu&amp;utm_medium=cpc&amp;utm_campaign=CP-APM&amp;utm_content=CJ&amp;utm_term=APM Testin听云：https://www.testin.cn/ 开源工具 Twitter Zipkin：http://zipkin.io/ （finagle 分布式微服务框架） 美团点评 CAT：https://link.jianshu.com/?t=https://github.com/dianping/cat 应用性能管理工具 PinPoint：https://link.jianshu.com/?t=https://github.com/naver/pinpoint Apache HTrace：https://link.jianshu.com/?t=http://htrace.incubator.apache.org/","tags":[]},{"title":"middle-service/apm/Skywalking/Skywalking环境搭建","date":"2020-02-22T08:01:23.266Z","path":"2020/02/22/middle-service/apm/Skywalking/Skywalking环境搭建/","text":"环境： Windows 7 x64 IDEA 2017.01 JDK 1.8 x64 Maven 3.5.0 源码获取： Skywalking Git可以利用 Git Clone 或者 ZIP 下载到本地； 利用 IDEA 选择 Maven 导入项目 项目结构 启动 Skywalking-collecor 在项目根目录下或者 IntelliJ IDEA Terminal 运行mvn clean compile install -Dmaven.test.skip=true； 编译完后设置 gRPC 自动生成代码的目录： apm-network/target/generated-sources/protobuf 下的grpc-java和java; GRPC设置 apm-collector/apm-collector-remote/collector-remote-grpc-provider/target/generated-sources/protobuf 下的grpc-java和java; gRPC设置 设置方法：在 grpc-java 上右键-&gt; Mark Directory as -&gt; Generated Souces Root 代码生成 运行org.skywalking.apm.collector.boot.CollectorBootStartUp的main(args)方法，启动 Collector； 在浏览器中输入http://127.0.0.1:10800/agent/jetty地址，返回 [&quot;localhost:12800/&quot;] ，说明启动成功。 启动 Skywalking-Agent 在 Skywalking 项目的同级新建一个 Web 项目 我这里新建了一个 SpringBoot 的项目（注意 SpringBootDemo 必须和 skywalking 项目平级，这样才可以调试 Agent） 项目结构 在org.skywalking.apm.agent.SkyWalkingAgent的premain()方法里打上断点； 在自己新建的Web项目，配置启动参数-javaagent:skywalking-agent.jar的路径（这里的路径可以绝对路径也可以是相对路径） agentJarPath web项目启动配置 启动Web项目，这里可能会出现agent.application_code is missing或者collector.servers is missing.的错误，我改了org.skywalking.apm.agent.core.conf.Config中的： 12public static String APPLICATION_CODE = \"SpringBootDemo\";public static String SERVERS = \"127.0.0.1:8080\"; 此时停掉之前启动的 skywalking-collector，重新运行mvn clean compile install -Dmaven.test.skip=true进行编译； 编译完成后再启动 skywalking-collector，然后启动自己的Web项目，如果程序进入了之前的org.skywalking.apm.agent.SkyWalkingAgent的premain()方法中的断点，并且 Web 项目启动成功则说明 Agent 模块启动成功。","tags":[]},{"title":"middle-service/微服务/微服务随记","date":"2020-02-22T07:58:59.550Z","path":"2020/02/22/middle-service/微服务/微服务随记/","text":"SpringCloud 学习示例项目 微服务特点 按业务划分为一个独立运行的程序，即服务单元 服务之间通过 http 协议通信 自动化部署 可用不同的编程语言 可用不同的存储技术 服务集中化管理 微服务是一个分布式系统微服务“微” 代码量 开发时间长短 业务大小CAP C（Consistency 一致性）：数据写入成功，之后读取读到的都是写入后的数据 A（Availability 可用性）：服务的可用性 P（Partition-tolerance 分区容错性）：单台或多台服务出现问题后，其他正常的服务仍可以正常提供服务微服务具有的功能 服务的注册和发现 服务的负载均衡 服务的容错 服务的网关 服务配置的统一管理 链路追踪 实时日志Zookeeper和Eureka Zookeeper Eureka CAP 满足CP 满足AP 服务注册 当 master 节点 down 掉，剩余节点会重新选举 leader ，耗时30~120s，选取期间整个集群不可用，服务瘫痪 各个节点平等，当请求一个节点失败时会自动切换至另一个节点，但是不保证各个节点的强一致性 Eureka 还有一种自我保护机制，如果在 15 分钟内超过 85% 的节点都没有正常的心跳，那么 Eureka 就认为客户端与注册中心出现了网络故障，此时会出现以下几种情况： 1.Eureka 不再从注册列表中移除因为长时间没收到心跳而应该过期的服务 2. Eureka 仍然能够接受新服务的注册和查询请求，但是不会被同步到其它节点上(即保证当前节点依然可用) 3.当网络稳定时，当前实例新的注册信息会被同步到其它节点中 负载均衡 将负载分摊到多个执行单元上，常见的两种方式 独立进程单元，通过负载均衡策略，将请求发送到不同的执行单元上，例如 Nginx 将负载均衡以代码逻辑的形式封装到服务消费者的客户端上，服务消费者维护了一份服务提供者的信息列表，通过负载均衡策略将请求分摊给多个服务提供者 Feign 简化 Java Http 客户端远程调用 Feign 采用的是http网络交互，可以采用feign-httpclient或者okhttp做网络请求框架，只需要在pom中添加相关依赖即可。 Feign中负载均衡也是通过Ribbon来实现的。 Feign请求过程 通过 @EnableFeignClients 注解开启FeignClient功能； 根据Feign的规则实现接口，并在接口上面加上 @FeignClient 注解； 程序启动自动扫描 @FeignClient 注解的类，并注入到IoC容器； 当接口的方法被调用时，通过JDK代理生成具体的 requestTemplate 对象，生成 http 请求的 request 对象； 将 request 对象交给 Client 去处理，可以使用 HttpUrlConnection 、HttpClient 或 OkHttp； 最后 Client 被封装到 LoadBalanceClient 类，这个类结合 Ribbon 实现负载均衡。 Feign 可以直接配置 Hystrix 熔断器 Hystrix 通过隔离服务的访问点组织联动故障，提供故障的解决方案，提高整个分布式系统的弹性 设计原则 防止单个服务的故障耗尽整个那个服务的 Servlet 容器的线程资源 快速失败机制，当某个服务出现故障，则调用该服务的线程快速失败而不是线程等待 提供回退方案，请求发生故障时，按照提供的方案回退 使用熔断机制，防止故障扩大影响到其他服务 利用监控组件实时监控熔断器的状态 Hystrix工作机制 当某个API服务在一定时间内失败的次数大于设定阀值，触发熔断器打开，这时请求该API服务的接口会执行快速失败逻辑（即回退方案）。 处于打开状态的熔断器，一段时间后，会处于半打开半关闭状态，将一定数量的请求执行正常逻辑，剩余的请求执行快速失败逻辑，如果执行正常逻辑的请求失败了，则熔断器继续打开，如果成功了，则熔断器关闭。 测试 执行请求中，断开两个服务提供方的其中一个，在短时间内的请求上仍然可以负载到 done 掉的服务上。在停止两个服务提供方后则会执行 fallback 配置的方法。 Zuul 作用 Zuul、Ribbon 以及 Eureka 结合，可以实现智能路由和负载均衡，Zuul能够将请求流量按照某种策略分发到集群状态的多个服务实例； 网关将所有的服务 API 接口统一聚合，并统一对外暴露。外界系统无需关注内部服务，也保护了内部的微服务单元避免敏感信息对外暴露； 服务网关做用户身份认证和权限控制，防止非法请求操作API接口； 网关实现监控功能，实时日志输出，对请求做记录； 网关可以实现流量监控，在高流量的情况下，实现降级； API 接口从内部服务分离处理，方便做测试 微服务链路追踪 Sleuth Dapper 中的专业术语： Span：基本工作单元，发送一个远程调用服务就会产生一个 Span，Span 是一个64位的 ID， Span 包含了摘要、时间戳事件、Span 的 ID 以及进程的 ID 。 Trace：有一系列 Span 组成，呈树状结构。请求一个微服务系统的 API 接口，这个 API 接口需要调用多个微服务单元，每调用一个新的微服务单元都会产生一个 Span ，所有由这个请求产生的 Span 组成了这个 Trace。 Annotation：用于记录一个事件，一些核心注解用户定义一个请求的开始和结束： cs-Client Sent：客户端发送一个请求，描述 Span 的开始 sr-Server Received：服务端获得请求并准备开始处理它，用 sr 减去 cs 时间戳，就是网络传输的时间 ss-Server Sent：服务端发送响应，该注解表明请求处理的完成，用 ss 减去 sr 的时间戳就是服务端处理的时间长度 cr-Client Received：客户端接受响应，此时 Span 结束，用 cr 减去 cs 的时间戳，就得到整个请求的耗时。 Zipkin 使用 RabbitMQ 传输链路数据，默认使用 Http 上传数据到 zipkin-server 的。 可以使用 MySql、Elasticsearch 存储链路数据，如果在Elasticsearch中存储数据，可以利用 Kibana 展示数据。 微服务监控 Spring Boot Security 安全 认证：认证主体，可以在应用程序中执行操作的用户、设备或其他系统 授权：拥有什么权限，允许已认证的主体执行某一项操作","tags":[]},{"title":"middle-service/mycat/Mycat基础","date":"2020-02-22T07:58:24.550Z","path":"2020/02/22/middle-service/mycat/Mycat基础/","text":"Mycat分布式数据库 透明性：不用关心逻辑分区和物理位置分布细节； 数据冗余性：通过冗余实现系统的可靠性、可用性。多节点存储数据副本，在某一节点损坏时，通过心跳机制，节点自动切换，保证系统可用。热点数据就近分布，减少网络损耗加快访问速度、提升性能； 易于扩展性：分布式数据库可以进行水平或者垂直进行扩展提高性能； 自治性：本节点上的数据由本地的DBMS管理； 分布式数据库实现原理分布式数据库的目录管理存放系统元数据以数据库元数据的全部信息，保证数据被有效、正确地访问 全局目录 分布式目录 全局与本地混合目录数据分片将一台数据库的压力分散到多台主机上，多台设备存取，提高性能、提高系统整体的可用性 水平切分：按照某个字段的某种规则将数据分散到多个节点库中 垂直切分：数据库由多表构成，每个表对应不同的业务，按照业务讲标分散不同的节点上。但是当表的数据量达到一定程度后，扩展较难 混合切分：将上述两者综合使用分布式查询处理将查询解析到各个数据节点，然后将结果汇总分布式并发控制并发控制保证分布式数据中的多个事务并发高效、正确的执行。并发控制保证事务的可串行性 加锁并发控制：容易死锁 时间戳控制：需要有全局的统一时钟，消除死锁，一旦发生冲突变回重启而不是等待 乐观并发控制：对于冲突较少的系统比较适合 Mycat架构、核心功能Mycat架构 mycat架构图 Mycat核心功能 mycat核心功能 Mycat核心概念逻辑库中间件被当做一个或多个数据库集群构成的逻辑库 逻辑表 分片表：将数据量很大的表切分到多个数据库实例中，例如将 mytable 配置到 dn1 和dn2 两个节点上1&lt;table name&#x3D;&quot;mytable&quot; primaryKey&#x3D;&quot;id&quot; autoIncrement&#x3D;&quot;true&quot; dataNode&#x3D;&quot;dn1,dn2&quot; rule&#x3D;&quot;myRule&quot; &#x2F;&gt; 非分片表：根据业务量，对于不需要分片的表可以只配置到一个节点上1&lt;table name&#x3D;&quot;mytable&quot; primaryKey&#x3D;&quot;id&quot; autoIncrement&#x3D;&quot;true&quot; dataNode&#x3D;&quot;dn1&quot; &#x2F;&gt; ER表：基于实体关系模型，子表的记录与其所关联的父表记录放在同一个数据分片上，保证关联查询不会跨分片 全局表：业务表规模较大分片后，业务表和附属字典表之间的关联查询比较麻烦，通过冗余数据解决这类关联查询，即所有分片都复制一份数据，这个冗余数据构成的表定义为全局表分片节点分片表被分到不同的分片数据库上，每个表所在的分片数据库就是分片节点节点主机将数据分片后，同一台机器上有可能有多个分片节点，所在的主机就是节点主机。为了规避单节点主机并发数量限制，尽量将读写压力高的节点合理分放，避免单节点压力过高 配置方式本地文件 schema.xml：mycat 的逻辑库、分片表、分片节点、分片节点主机信息配置 server.xml：系统参数配置文件，mycat 优化相关属性 rule.xml：分片规则配置文件 log4j2.xml：mycat 输出日志配置文件 sequence.properties：全局序列配置文件Zookeeper 配置 zk-create.yaml：本地文件中的配置项都配置在此文件中","tags":[]},{"title":"middle-service/mycat/Mycat 分片配置","date":"2020-02-22T07:58:23.325Z","path":"2020/02/22/middle-service/mycat/Mycat 分片配置/","text":"Mycat 分片配置mycat 将表分为两种概念：对于数据量小且不需要做数据切分的表成为非分片表；数据量大到单库性能、容量不足以支撑，数据需要通过水平切分均匀分布到不同的数据库表称之为分片表 rule.xmlrule.xml 为分片规则配置文件， schema.xml 中 table 标签中的 rule 属性的值需要在 rule.xml 中配置 function 标签123&lt;function name&#x3D;&quot;mod-long&quot; class&#x3D;&quot;io.mycat.route.function.PartitionByMod&quot;&gt; &lt;property name&#x3D;&quot;count&quot;&gt;3&lt;&#x2F;property&gt;&lt;&#x2F;function&gt; name 指定算法的名称，在该文件中唯一 class 属性对应具体的分片算法，指定具体的算法类 property 根据算法要求指定tableRule 标签123456&lt;tableRule name&#x3D;&quot;my-mod-long&quot;&gt; &lt;rule&gt; &lt;columns&gt;id&lt;&#x2F;columns&gt; &lt;algorithm&gt;mod-long&lt;&#x2F;algorithm&gt; &lt;&#x2F;rule&gt;&lt;&#x2F;tableRule&gt; name 指定分片唯一算法名称 rule 分片算法具体内容 columns 对应表中用于分片的列名 algorithm function 中定义的算法名称取模分片123456789&lt;tableRule name&#x3D;&quot;my-mod-long&quot;&gt; &lt;rule&gt; &lt;columns&gt;id&lt;&#x2F;columns&gt; &lt;algorithm&gt;mod-long&lt;&#x2F;algorithm&gt; &lt;&#x2F;rule&gt;&lt;&#x2F;tableRule&gt;&lt;function name&#x3D;&quot;mod-long&quot; class&#x3D;&quot;io.mycat.route.function.PartitionByMod&quot;&gt; &lt;property name&#x3D;&quot;count&quot;&gt;3&lt;&#x2F;property&gt;&lt;&#x2F;function&gt; 概算法根据 ID 进行十进制求模计算，相比固定的分片 hash，这种分片算法在批量插入会增加事务一致性的难度枚举分片通过在配置文件中配置可能的枚举ID，指定数据分布到不同的物理节点上，本规则适合按照省份或县区来拆分数据类业务1234567891011&lt;tableRule name&#x3D;&quot;sharding-byintfile&quot;&gt; &lt;rule&gt; &lt;columns&gt;sharding_id&lt;&#x2F;columns&gt; &lt;algorithm&gt;hash-init&lt;&#x2F;algorithm&gt; &lt;&#x2F;rule&gt;&lt;&#x2F;tableRule&gt;&lt;function name&#x3D;&quot;hash-init&quot; class&#x3D;&quot;io.mycat.route.function.PartitionByFileMap&quot;&gt; &lt;property name&#x3D;&quot;mapFile&quot;&gt;partion-hash-init.txt&lt;&#x2F;property&gt; &lt;property name&#x3D;&quot;type&quot;&gt;0&lt;&#x2F;property&gt; &lt;property name&#x3D;&quot;defaultNode&quot;&gt;0&lt;&#x2F;property&gt;&lt;&#x2F;function&gt; 其中 partition-hash-init.txt 内容：12310000&#x3D;010010&#x3D;1DEFAULT_NODE&#x3D;1 type 默认值为 0，0 表示 Integer，非 0 表示 String defaultNode 小于 0 表示不设置默认节点，大于等于 0 表示设置默认节点。默认节点的作用：枚举分片时，如果有不认识的枚举值就路由到默认节点。如果不配置默认节点，遇到无法识别的枚举值，就会报错。范围分片12345678910&lt;tableRule name&#x3D;&quot;auto-sharding-long&quot;&gt; &lt;rule&gt; &lt;columns&gt;id&lt;&#x2F;columns&gt; &lt;algorithm&gt;range-long&lt;&#x2F;algorithm&gt; &lt;&#x2F;rule&gt;&lt;&#x2F;tableRule&gt;&lt;function name&#x3D;&quot;range-long&quot; class&#x3D;&quot;io.mycat.route.function.AutoPartitionByLong&quot;&gt; &lt;property name&#x3D;&quot;mapFile&quot;&gt;auto-partition-long.txt&lt;&#x2F;property&gt; &lt;property name&#x3D;&quot;defaultNode&quot;&gt;0&lt;&#x2F;property&gt;&lt;&#x2F;function&gt; auto-partition-long.txt 配置如下：1234# range start-end，data node index0-10000&#x3D;010001-20000&#x3D;120000-60000&#x3D;2 范围求模算法先范围分片，然后组内求模。范围求模可以保证组内数据分布比较均匀，避免热点数据问题；范围分片在水平扩展时，原有数据不需要迁移。12345678910&lt;tableRule name&#x3D;&quot;auto-sharding-rang-mod&quot;&gt; &lt;rule&gt; &lt;columns&gt;id&lt;&#x2F;columns&gt; &lt;algorithm&gt;range-mod&lt;&#x2F;algorithm&gt; &lt;&#x2F;rule&gt;&lt;&#x2F;tableRule&gt;&lt;function name&#x3D;&quot;range-mod&quot; class&#x3D;&quot;io.mycat.route.function.PartitionByRangeMod&quot;&gt; &lt;property name&#x3D;&quot;mapFile&quot;&gt;partition-range-mod.txt&lt;&#x2F;property&gt; &lt;property name&#x3D;&quot;defaultNode&quot;&gt;0&lt;&#x2F;property&gt;&lt;&#x2F;function&gt; partiton-range-mod.txt 配置如下，等号前面的范围代表一个分片组，等号后面的数字代表该分片组所拥有的分片数量1230-200M&#x3D;5 &#x2F;&#x2F;5个分片节点200M1-400M&#x3D;1400M1-600M&#x3D;3 固定分片 hash 算法12345678910&lt;tableRule name&#x3D;&quot;sharding-hash&quot;&gt; &lt;rule&gt; &lt;columns&gt;userId&lt;&#x2F;columns&gt; &lt;algorithm&gt;sharding-hash&lt;&#x2F;algorithm&gt; &lt;&#x2F;rule&gt;&lt;&#x2F;tableRule&gt;&lt;function name&#x3D;&quot;sharding-hash&quot; class&#x3D;&quot;io.mycat.route.function.PartitionByLong&quot;&gt; &lt;property name&#x3D;&quot;partitionCount&quot;&gt;2,1&lt;&#x2F;property&gt; &lt;property name&#x3D;&quot;partitionLength&quot;&gt;256,512&lt;&#x2F;property&gt;&lt;&#x2F;function&gt; partitionCount 分片个数列表 partitionLength 分片范围列表，分区长度默认最大为 2^n = 1024，最大支持 1024 个分区 count 和 length 两个数组的长度必须一致。1024 = SUM((count[i]*length[i]))，count 和 length两个向量的点击恒等于 1024。 取模范围算法先取模，然后范围分片 1234567891011&lt;tableRule name&#x3D;&quot;sharding-by-pattern&quot;&gt; &lt;rule&gt; &lt;columns&gt;userId&lt;&#x2F;columns&gt; &lt;algorithm&gt;sharding-by-pattern&lt;&#x2F;algorithm&gt; &lt;&#x2F;rule&gt;&lt;&#x2F;tableRule&gt;&lt;function name&#x3D;&quot;sharding-by-pattern&quot; class&#x3D;&quot;io.mycat.route.function.PartitionByPattern&quot;&gt; &lt;property name&#x3D;&quot;patternValue&quot;&gt;256&lt;&#x2F;property&gt; &lt;property name&#x3D;&quot;defaultNode&quot;&gt;2&lt;&#x2F;property&gt; &lt;property name&#x3D;&quot;mapFile&quot;&gt;partition-pattern.txt&lt;&#x2F;property&gt;&lt;&#x2F;function&gt; partition-pattern.txt 配置，dataNode 是默认节点，如果采用默认配置则不进行求模运算，如果 id 不是数据则会分配在默认节点上 12341-32&#x3D;033-64&#x3D;165-96&#x3D;297-128&#x3D;3 patternValue 求模基数字符串 hash 求模范围算法与上相同，该算法支持述职、符号、字母取模123456789101112&lt;tableRule name&#x3D;&quot;sharding-by-prefixpattern&quot;&gt; &lt;rule&gt; &lt;columns&gt;userId&lt;&#x2F;columns&gt; &lt;algorithm&gt;sharding-by-prefixpattern&lt;&#x2F;algorithm&gt; &lt;&#x2F;rule&gt;&lt;&#x2F;tableRule&gt;&lt;function name&#x3D;&quot;sharding-by-prefixpattern&quot; class&#x3D;&quot;io.mycat.route.function.PartitionByPrefixPattern&quot;&gt; &lt;property name&#x3D;&quot;patternValue&quot;&gt;256&lt;&#x2F;property&gt; &lt;property name&#x3D;&quot;prefixLength&quot;&gt;5&lt;&#x2F;property&gt; &lt;property name&#x3D;&quot;defaultNode&quot;&gt;2&lt;&#x2F;property&gt; &lt;property name&#x3D;&quot;mapFile&quot;&gt;partition-prefixpattern.txt&lt;&#x2F;property&gt;&lt;&#x2F;function&gt; partition-prefixpattern.txt，截取长度为 prefixLength 的子串，再对字符串中每个字符的 ASCII 码进行求和得出 sum 值，最后对 sum 进行求模运算 (sum % patternValue)，可以算出分片数12341-4&#x3D;05-8&#x3D;19-12&#x3D;213-16&#x3D;3 应用指定的算法1234567891011&lt;tableRule name&#x3D;&quot;sharding-by-substring&quot;&gt; &lt;rule&gt; &lt;columns&gt;userId&lt;&#x2F;columns&gt; &lt;algorithm&gt;sharding-by-substring&lt;&#x2F;algorithm&gt; &lt;&#x2F;rule&gt;&lt;&#x2F;tableRule&gt;&lt;function name&#x3D;&quot;sharding-by-substring&quot; class&#x3D;&quot;io.mycat.route.function.PartitionDirectBySubString&quot;&gt; &lt;property name&#x3D;&quot;patternCount&quot;&gt;8&lt;&#x2F;property&gt; &lt;property name&#x3D;&quot;size&quot;&gt;2&lt;&#x2F;property&gt; &lt;property name&#x3D;&quot;defaultPattern&quot;&gt;0&lt;&#x2F;property&gt;&lt;&#x2F;function&gt; 根据字符串子串(必须是数字)计算分区号，例如 id = 05-0001，其中 id 是从 startIndex = 0 开始，截取长度为 2 位，即分区号为 05，分配到默认分区。字符串 hash 解析算法1234567891011&lt;tableRule name&#x3D;&quot;sharding-by-stringhash&quot;&gt; &lt;rule&gt; &lt;columns&gt;userId&lt;&#x2F;columns&gt; &lt;algorithm&gt;sharding-by-stringhash&lt;&#x2F;algorithm&gt; &lt;&#x2F;rule&gt;&lt;&#x2F;tableRule&gt;&lt;function name&#x3D;&quot;sharding-by-stringhash&quot; class&#x3D;&quot;io.mycat.route.function.PartitionByString&quot;&gt; &lt;property name&#x3D;&quot;length&quot;&gt;512&lt;&#x2F;property&gt; &lt;property name&#x3D;&quot;count&quot;&gt;2&lt;&#x2F;property&gt; &lt;property name&#x3D;&quot;hashSlice&quot;&gt;0:2&lt;&#x2F;property&gt;&lt;&#x2F;function&gt; length 字符串 hash 的求模基数 count 分区数 hashSlice 预算位，根据子字符串中的 int 值进行 hash 运算一致性 hash 算法12345678910111213&lt;tableRule name&#x3D;&quot;sharding-by-murmur&quot;&gt; &lt;rule&gt; &lt;columns&gt;userId&lt;&#x2F;columns&gt; &lt;algorithm&gt;sharding-by-murmur&lt;&#x2F;algorithm&gt; &lt;&#x2F;rule&gt;&lt;&#x2F;tableRule&gt;&lt;function name&#x3D;&quot;sharding-by-murmur&quot; class&#x3D;&quot;io.mycat.route.function.PartitionByMurMueHash&quot;&gt; &lt;property name&#x3D;&quot;seed&quot;&gt;0&lt;&#x2F;property&gt; &lt;property name&#x3D;&quot;count&quot;&gt;2&lt;&#x2F;property&gt; &lt;property name&#x3D;&quot;virtualBucketTimes&quot;&gt;160&lt;&#x2F;property&gt; &lt;property name&#x3D;&quot;weightMapFile&quot;&gt;weightMapFile&lt;&#x2F;property&gt; &lt;property name&#x3D;&quot;bucketMapPath&quot;&gt;&#x2F;etc&#x2F;mycat&#x2F;bucketMapPath&lt;&#x2F;property&gt;&lt;&#x2F;function&gt; count 要分片的数据库节点数量 virtualBucketTimes 一个实际的数据库节点被映射后的虚拟节点，默认是 160 倍，即虚拟节点是物理节点的 160 倍 weightMapFile 节点的权重，没有指定权重的节点默认是 1，以 properties 文件的格式编辑，以从 0 开始到 count - 1 的整数值也就是节点索引 key，以节点权重值为值。所有权重必须是正整数，否则以 1 代替 bucketMapPath 测试时观察各物理节点与虚拟节点的分布情况。如果指定该属性，则会把虚拟节点的 murmur hash 值与物理节点的映射输出到这个文件，没有默认值，不配置则不输出。按日期(天)分片算法123456789101112&lt;tableRule name&#x3D;&quot;sharding-by-date&quot;&gt; &lt;rule&gt; &lt;columns&gt;create_time&lt;&#x2F;columns&gt; &lt;algorithm&gt;sharding-by-date&lt;&#x2F;algorithm&gt; &lt;&#x2F;rule&gt;&lt;&#x2F;tableRule&gt;&lt;function name&#x3D;&quot;sharding-by-date&quot; class&#x3D;&quot;io.mycat.route.function.PartitionByDate&quot;&gt; &lt;property name&#x3D;&quot;dateFormat&quot;&gt;yyyy-MM-dd&lt;&#x2F;property&gt; &lt;property name&#x3D;&quot;sBeginDate&quot;&gt;2019-08-08&lt;&#x2F;property&gt; &lt;property name&#x3D;&quot;sEndDate&quot;&gt;2019-08-09&lt;&#x2F;property&gt; &lt;property name&#x3D;&quot;sPartionDay&quot;&gt;10&lt;&#x2F;property&gt;&lt;&#x2F;function&gt; sPartitionDay 分区天数，默认从开始日期算起，每隔 10 天一个分区 sEndDate 如果配置该属性，则数据达到这个日期的分片后会重复从开始分片插入单月小时算法123456789&lt;tableRule name&#x3D;&quot;sharding-by-hour&quot;&gt; &lt;rule&gt; &lt;columns&gt;create_time&lt;&#x2F;columns&gt; &lt;algorithm&gt;sharding-by-hour&lt;&#x2F;algorithm&gt; &lt;&#x2F;rule&gt;&lt;&#x2F;tableRule&gt;&lt;function name&#x3D;&quot;sharding-by-hour&quot; class&#x3D;&quot;io.mycat.route.function.LatestMonthPartition&quot;&gt; &lt;property name&#x3D;&quot;sliptOneDay&quot;&gt;24&lt;&#x2F;property&gt;&lt;&#x2F;function&gt; columns 为拆分字段，字符串类型(yyyyMMddHH) sliptOneDay 为一天切分的分片数自然月分片算法12345678910&lt;tableRule name&#x3D;&quot;sharding-by-month&quot;&gt; &lt;rule&gt; &lt;columns&gt;create_time&lt;&#x2F;columns&gt; &lt;algorithm&gt;sharding-by-month&lt;&#x2F;algorithm&gt; &lt;&#x2F;rule&gt;&lt;&#x2F;tableRule&gt;&lt;function name&#x3D;&quot;sharding-by-month&quot; class&#x3D;&quot;io.mycat.route.function.PartitionByMonth&quot;&gt; &lt;property name&#x3D;&quot;dateFormat&quot;&gt;yyyy-MM-dd&lt;&#x2F;property&gt; &lt;property name&#x3D;&quot;sBeginDate&quot;&gt;2019-08-08&lt;&#x2F;property&gt;&lt;&#x2F;function&gt; 日期范围 hash 算法现根据日期分组，再根据时间 hash 使得短期内数据分布更加均匀，可在一定程度上避免范围分片的数据热点问题123456789101112&lt;tableRule name&#x3D;&quot;range-date-hash&quot;&gt; &lt;rule&gt; &lt;columns&gt;create_time&lt;&#x2F;columns&gt; &lt;algorithm&gt;range-date-hash&lt;&#x2F;algorithm&gt; &lt;&#x2F;rule&gt;&lt;&#x2F;tableRule&gt;&lt;function name&#x3D;&quot;range-date-hash&quot; class&#x3D;&quot;io.mycat.route.function.PartitionByRangeDateHash&quot;&gt; &lt;property name&#x3D;&quot;dateFormat&quot;&gt;yyyy-MM-dd&lt;&#x2F;property&gt; &lt;property name&#x3D;&quot;sBeginDate&quot;&gt;2019-08-08&lt;&#x2F;property&gt; &lt;property name&#x3D;&quot;sPartionDay&quot;&gt;12&lt;&#x2F;property&gt; &lt;property name&#x3D;&quot;groupPartionSize&quot;&gt;6&lt;&#x2F;property&gt;&lt;&#x2F;function&gt; sPartionDay 代表多少天一组 groupPartionSize 每组的分片数量","tags":[]},{"title":"middle-service/mycat/Tips","date":"2020-02-22T07:58:21.802Z","path":"2020/02/22/middle-service/mycat/Tips/","text":"高可用 VIP高可用性HA (High Availability) 尽量缩短系统日常的维护操作和突发的系统奔溃导致的停机时间，提高系统和应用的可用性。数据库通常通过主从关系来实现。 数据库实现高可用，数据库服务搭建主从关系，主库 IP、从库 IP、读写 VIP、只读 VIP，使用这几个 IP 地址都可以连接对应的数据库服务。VIP 如何寻址到对应的服务器，使用的是 TCP/IP 协议的 ARP 协议。当 物理数据库无法使用时，动态将 VIP 对应的 MAC 地址替换成从库","tags":[{"name":"高可用","slug":"高可用","permalink":"https://zcy-fover.github.io/tags/%E9%AB%98%E5%8F%AF%E7%94%A8/"}]},{"title":"middle-service/mycat/配置文件","date":"2020-02-22T07:58:20.328Z","path":"2020/02/22/middle-service/mycat/配置文件/","text":"配置文件server.xmluser 标签1234567&lt;user name&#x3D;&quot;zcy-fover&quot;&gt; &lt;property name&#x3D;&quot;schemas&quot;&gt;mycattest1,mycatttest2&lt;&#x2F;property&gt; &lt;property name&#x3D;&quot;password&quot;&gt;123456&lt;&#x2F;property&gt; &lt;property name&#x3D;&quot;readOnly&quot;&gt;true&lt;&#x2F;property&gt; &lt;property name&#x3D;&quot;benchmark&quot;&gt;100&lt;&#x2F;property&gt; &lt;property name&#x3D;&quot;usingDecrypt&quot;&gt;1&lt;&#x2F;property&gt;&lt;&#x2F;user&gt; user 标签用于定义登录 mycat 的用户和权限，配置的用户只可以访问对应的 schema schemas 可以配置多个 schema 用英文逗号分隔 readOnly 属性用来限制用户的读写权限 benchmark 属性限制前端的整体链接数量，置为 0 或者不设值则表示不限 usingDecrypt 属性表示开启密码加密功能，0：不开启，1：开启 system 标签 charset 字符集属性，配置时保证与 mysql 的一致 defaultSqlParser 指定默认的 sql 解析器 processors 指定系统可用的线程数量，默认值为机器 CPU 核心 x 每个核心运行线程的数量，processors 的值会影响 processorsBufferPool、processorsBufferLocalPersent、ProcessorsExecutor 和 NIOProcessor 属性 processorsBufferChunk 指定每次分配 Socket Direct Buffer 的默认字节是 4096，也会影响 bufferPool 的长度，如果一次性获取的字节过多导致 buffer 不够用，会出现告警，可以适当提高 processorsBufferChunk 的值 processorsBufferPool 指定 bufferPool 的计算比例，由于每次执行 NIO 读写都要使用到 buffer，所以 mycat 在初始化是会创建一定长度的 buffer 池来加快 NIO 读写效率，减少 buffer 的时间。 processorsBufferPool 的默认值：bufferChunkSize(4096) * processers * 1000 BufferPool 和 ThreadLocalPool 池，BufferPool 使用 ThreadLocalPool 作为二级缓存，每次从 BufferPool 获取时都会优先从 ThreadLocalPool 中获取 Buffer 的值，如果未命中，则会从 BufferPool 中获取，ThreadLocalPool 在每个线程内部使用，而 BufferPool 是每个 NIO 共享的。 BufferPool 的总长度为 BufferPool / BufferChunk，如果比值不是整数倍，则取整加一 processorBufferLocalPercent 控制 ThreadLocalPool 分配 Pool 的比例大小，该属性的默认值为 100 线程缓存百分比 = BufferLocalPercent / processors ThreadLocalPool 长度 = 线程缓存百分比 * BufferPool 长度 / 100 processorExecutor 指定 NIOProcessor 上共享 businessExecutor 固定线程池的大小，Mycat 把异步处理任务提交到这个 businessExecutor 线程池中 sequenceHandlerType 指定 Mycat 全局序列的类型，0：本地文件方式，1：数据库方式，2：时间戳序列方式。默认使用本地文件方式 TCP连接的相关属性 StandardSocketOptions.SO_RCVBUF、StandardSocketOptions.SO_SNDBUF、StandardSocketOptions.TCP_NODELAY对应到 mycat 的 TCP 连接配置如下： frontSocketSoRcvbuf：默认值为 1024 * 1024 frontSocketSoSndbuf：默认值为 4 * 1024 *1024 frontSocketSoNoDelay：默认值为 1 backSocketSoRcvbuf：默认值为 4 * 1024 * 1024 backSocketSoSndbuf：默认值为 1024 *1024 backSocketSoNoDelay：默认值为 1 MySQL连接的相关属性 packetHeaderSize：指定 MySQL 协议中的报文长度，默认值为 4 个字节 maxPacketSize：指定 MySQL 协议可以携带的数据的最大值，默认值为 16MB idleTimeout：指定连接的空闲时间的超时长度。如果某个连接的空闲时间超过 idleTimeout 的值，则该连接资源将被关闭并回收，单位毫秒，默认 30 分钟 charset：初始化连接字符集，默认 utf8 txIsolation：初始化前端连接事务的隔离级别，后续的 txIsolation 值为客户端的配置值。默认为 REPEATED_READ；READ_UNCOMMITTED: 1、READ_COMMITTED: 2、REPEATED_READ: 3、SERIALIZABLE: 4 sqlExecuteTimeout：执行 SQL 语句的超时时间，若 SQL 执行超过这个时间，则会关闭连接，单位为秒，默认 300 秒 心跳属性 processorCheckPeriod：清理 NIOProcessor 前后端连接空闲、超时、关闭连接的时间间隔，单位为毫秒，默认为 1 秒 dataNodeIdleCheckPeriod：对后端连接进行空闲、超时检查的时间间隔，单位为毫秒默认为 300 秒 dataNodeHeartbeatPeriod：对后端的所有读、写库发起心跳的时间间隔，单位为毫秒，默认为 10 秒 服务相关属性 bindIp：服务监听的 IP 地址，默认值为 0.0.0.0 serverPort：定义 mycat 的使用端口，默认值为 8066 managerPort：定义 mycat 的管理端口，默认值为 9066 fakeMySQLVersion属性 mycat 使用 MySQL 的通信协议模拟了一个 MySQL 服务器，默认版本是 5.6 handleDistributedTransactions 分布式事务开关属性, 值为0：不过滤分布式事务 值为1：过滤分布式事务（如果分布式事务内只涉及全局表，则不过滤） 值为2：不过滤分布式无，但是记录分布式事务日志 useOffHeapForMerge 该属性指定是否启用非堆内存处理跨分片结果集，1：开启，0：关闭 useGlobleTableCheck 全局表一致性检查，1：开启，0：关闭 useSQLStat 开启实时统计，1：开启，0：关闭 useCompression 是否开启 MySQL 压缩协议，1：开启，0：关闭 usingAIO 0：NIO，1：AIOfirewall标签全局防火墙的设置，针对IP地址进行限制12345678&lt;firewall&gt; &lt;whitehost&gt; &lt;host host=\"127.0.0.1\" user=\"mycat\"/&gt; &lt;host host=\"127.0.0.2\" user=\"mycat\"/&gt; &lt;/whitehost&gt; &lt;blacklist check=\"false\"&gt; &lt;/blacklist&gt;&lt;/firewall&gt; schema.xmlmycat 的逻辑库、表、分片规则、分片节点及数据源相关配置项schema 标签定义逻辑库123456&lt;schema name&#x3D;&quot;test1&quot; checkSQLschema&#x3D;&quot;false&quot; sqlMaxLimit&#x3D;&quot;100&quot;&gt; &lt;table name&#x3D;&quot;goods_order&quot; subTables&#x3D;&quot;goods_order$0-2&quot; primaryKey&#x3D;&quot;ID&quot; autoIncrement&#x3D;&quot;true&quot; dataNode&#x3D;&quot;dn1&quot; rule&#x3D;&quot;my-mod-long&quot;&#x2F;&gt;&lt;&#x2F;schema&gt;&lt;schema name&#x3D;&quot;test2&quot; checkSQLschema&#x3D;&quot;false&quot; sqlMaxLimit&#x3D;&quot;100&quot;&gt; &lt;table name&#x3D;&quot;goods_order&quot; subTables&#x3D;&quot;goods_order$0-2&quot; primaryKey&#x3D;&quot;ID&quot; autoIncrement&#x3D;&quot;true&quot; dataNode&#x3D;&quot;dn1&quot; rule&#x3D;&quot;my-mod-long&quot;&#x2F;&gt;&lt;&#x2F;schema&gt; name schema名称，需要在 server.xml 中定义后才可以 dataNode 配置逻辑库的默认分片，没有配置 table 标签则会分到默认的 dataNode；通过 mycat 建表，没有提前配置 table 标签，也没有配置默认 dataNode，则会报错 checkSQLSchema 当值为 true 时，sql 语句发送到数据库执行时，会去掉 schema，例如SELECT * FROM test1.user会变成SELECT * FROM user，如果语句所带的 schema 没有在schema标签中指定，则 mycat 不会去掉，在执行时如果没有定义该库则执行会出错。 sqlMaxLimit 拆分库情况下，限制返回数据的大小。例如设置为 100，当实际 sql 中没有使用 limit，在执行时，mycat 会自动加上 limit 100。如果在实际 sql 中写了 limit 语句，则该属性即时设置也无效。table 标签定义逻辑表1&lt;table name=\"goods_order\" subTables=\"goods_order$0-2\" primaryKey=\"ID\" autoIncrement=\"true\" dataNode=\"dn1\" rule=\"my-mod-long\"/&gt; name 定义逻辑表的名称，和数据库中执行 create table 语句一样，同一个 schema 标签中，定义的表名必须唯一 dataNode 定义逻辑表所属的 dataNode，该值需要和 dataNode 标签中定义的一样，如果该表分布在多个 dataNode 上，可以如上类似使用 $ 符减少配置 rule 定义逻辑表使用的分表规则，该值需要在 rule.xml 文件中定义且需对应 ruleRequired 指定表是否绑定分片规则，如果设为true，但是没有指定 rule，则会报错 primaryKey 逻辑表对应真实表的主键。如果该属性配置的是真实表的主键，mycat 会缓存主键与 dataNode 的信息，再次使用主键查询时就不会广播式查询，直接路由到对应的 dataNode。但是如果缓存没有命中，还是会进行广播式查询。 type 定义逻辑表的类型，全局表：type 值是global；普通表：不指定该值为 global 的表 autoIncrement MySQL 对于非自增主键使用 last_insert_id() 不会返回结果，只会返回0，所以在 mycat 中使用该属性需要 MySQL 的表主键定义 auto_increment。使用该属性时配合数据库模式的全局序列使用 subTables 定义子表名称，目前在 mycat 1.6 版本后才支持分表，并且 dataNode 在分表条件下只能配置一个 needAddLimit 指定表是否需要在每个语句的后面加上 limit 限制，添加该属性后 mycat 会默认为查询语句后面添加 limit 100，如果 sql 语句中添加了 limit 限制，则该属性失效。该属性默认值为 true。childTable 标签定义 E-R 分片的子表，通过标签上的属性与父表关联，将有关联关系的父、子表放在同一个节点上，方便查询提高效率 name 子表名称 joinKey 插入子表时使用该属性查找父表存储的数据节点；该属性为子表的属性 parentKey 与父表建立关联关系的列名。首先获取 joinKey 再通过 parentKey 指定的列名产生查询语句，通过执行该查询语句知道父表在哪个分片上，从而确定子表的的存储位置；该属性是父表的属性 primaryKey 同 table 标签 needAddLimit 同 table 标签dataNode 标签定义 mycat 中的数据节点，一个 dataNode 标签就是一个独立的数据分片节点1&lt;dataNode name=\"dn1\" dataHost=\"mysql1\" database=\"mycattest1\"/&gt; name 定义 dataNode 的唯一名字，应用在 table 标签上的 dataNode 属性，建立表和数据节点的对应关系 dataHost 定义该数据分片所属的数据库实例，需要在 dataHost 标签中定义 dataBase 定义该分片所属数据库实例上的具体库，利用 实例+具体的库 两个维度定义分片，每个库上的表结构是一样的，这样可以方便对表进行水平拆分dataHost 标签定义数据库实例、读写分离和心跳123456&lt;dataHost name=\"mysql1\" maxCon=\"1000\" minCon=\"20\" balance=\"0\" writeType=\"0\" dbType=\"mysql\" dbDriver=\"native\" switchType=\"1\" slaveThreshold=\"100\"&gt; &lt;heartbeat&gt;select user()&lt;/heartbeat&gt; &lt;writeHost host=\"hostM0\" url=\"127.0.0.1:3306\" user=\"test\" password=\"123456\"&gt; &lt;readHost host=\"hostS0\" url=\"127.0.0.1:3306\" user=\"test\" password=\"123456\"/&gt; &lt;/writeHost&gt;&lt;/dataHost&gt; name 定义 dataHost 标签的名称，在 dataNode 中使用 maxCon 每个读写实例连接池的最大连接数，内嵌标签 writeHost 和 readHost 都会使用这个值来初始化连接池的最大长度 minCon 每个读写实例连接池的最小连接数，初始化连接池大小 balance 负载均衡类型 balance=”0”：不开启读写分离机制，所有读操作发送到当前可用的 writeHost 上 balance=”1”：全部的 readHost 与 stand by writeHost 都参与当前 select 语句的负载均衡。即当为双主双从模式(M1-&gt;S1，M2-&gt;S2，并且M1，M2互为主备)，正常情况下 M2、S1、S2都参与 select 语句的负载均衡 balance=”2”：所有的读操作都随机地分在 writeHost 和 readHost 上 balance=”3”：所有的读请求都随机分发到 writeHost 对应的 readHost 上，writeHost 不负载读压力。该配置在 mycat 1.3 后才有 dbType 指定后端连接的数据库类型，支持二进制的 MySQL 协议，还可以用 JDBC 连接 MongoDB、Oracle 等 dbDriver 指定连接后端使用的 Driver，可选 native 和 jdbc，因为 native 执行的是二进制的 MySQL 协议，所以可以使用 MySQL 和 MariaDB。其他类型的数据库需要使用 jdbc 驱动来支持，如果使用 jdbc，需要把支持 jdbc4 标准的驱动 jar 包放到 mycat 的 lib 文件夹下 switchType -1：不自动切换 1：默认值，表示自动切换 2：基于 MySQL 主从同步的状态决定是否切换，心跳语句 show slave status 3：基于 MySQL Galary Cluster 的切换机制(适合集群，用于 mycat 1.4.1及上)，心跳语句show status like &#39;wsrep%&#39; tempReadHostAvailable 如果配置了 writeHost，下面的 readHost 依旧可用，默认值为 0heartBeat 标签指明用于后端数据库心跳检查的语句，例如 MySQL 可以使用select user()，Oracle 可以使用select 1 from dual，这个标签还有 connectionInitSql 属性，当使用 Oracle 需要执行初始化 SQL 的语句放到这里。writeHost、readHost 标签实例化后端连接池 host 标识不同的实例，writeHost 通常用 *M1；readHost 通常用 *S1 url 实例数据库连接地址，如果使用的是 native，一般为 address:port；使用 JDBC 或其他 dbDriver，需要自己指定；例如 JDBC示例：jdbc:MySQL://localhost:3306 user 后端存储实例的用户名 password 后端存储实例的密码 weight 在 readHost 中作为读节点的权重 usingDecrypt 开启密码加密功能，0：不开启，1：开启rule.xml分片规则配置文件， schema.xml 中 table 标签中的 rule 属性的值需要在 rule.xml 中配置function 标签123&lt;function name=\"mod-long\" class=\"io.mycat.route.function.PartitionByMod\"&gt; &lt;property name=\"count\"&gt;3&lt;/property&gt;&lt;/function&gt; name 指定算法的名称，在该文件中唯一 class 属性对应具体的分片算法，指定具体的算法类 property 根据算法要求指定tableRule 标签123456&lt;tableRule name=\"my-mod-long\"&gt; &lt;rule&gt; &lt;columns&gt;id&lt;/columns&gt; &lt;algorithm&gt;mod-long&lt;/algorithm&gt; &lt;/rule&gt;&lt;/tableRule&gt; name 指定分片唯一算法名称 rule 分片算法具体内容 columns 对应表中用于分片的列名 algorithm function 中定义的算法名称sequence 配置文件分库分表情况下，数据库自增主键无法保证主键在集群中全局唯一，mycat 提供本地配置和数据库配置本地文件方式要启用这种方式，需要在 server.xml 配置文件中配置如下参数： name1234567891011121314151617181920212223配置后，在 mycat 的conf目录下，在 sequence_conf.properties 配置如下参数：&#96;&#96;&#96;propertiesCOMPANY.MAXID&#x3D;2000GLOBAL.MAXID&#x3D;20000COMPANY.HISIDS&#x3D;CUSTOMER.MAXID&#x3D;2000HOTNEWS.CURID&#x3D;1000ORDER.MINID&#x3D;1001CUSTOMER.HISIDS&#x3D;HOTNEWS.MINID&#x3D;1001GLOBAL.CURID&#x3D;10054ORDER.MAXID&#x3D;2000COMPANY.CURID&#x3D;1000CUSTOMER.CURID&#x3D;1000COMPANY.MINID&#x3D;1001GLOBAL.MINID&#x3D;10001HOTNEWS.MAXID&#x3D;2000CUSTOMER.MINID&#x3D;1001GLOBAL.HISIDS&#x3D;HOTNEWS.HISIDS&#x3D;ORDER.HISIDS&#x3D;ORDER.CURID&#x3D;1000 mycat 重新发布后配置文件中的 sequence 会恢复到初始值；优点是本地文件加载且读取速度快 数据库方式在数据库中创建一张名为 sequence 的表，在 sequence_db_conf.properties 进行相关配置 sequence 获取步骤 初次使用 sequence，根据传入的 sequence 民称从数据表中读取 current_value、increment 到 mycat 中，并做 current_value = current_value + increment mycat 将读取到的 current_value + increment 作为本次使用的 sequence 值，下次使用时 sequence 自动加一，当使用 increment 次后，执行与步骤一相同的操作 mycat 负责维护这张表，用到那些 sequence 时，只需要在这张表中插入一条记录即可。若某次读取的 sequence 没有用完，系统宕机了，则本次已经读取未使用的 sequence 值会被丢弃 数据库配置 创建表 123456789#创建 sequence 表CREATE TABLE IF NOT EXISTS sequence ( name VARCHAR(50) NOT NULL, current_value INT NOT NULL, increment INT NOT NULL DEFAULT 100, PRIMARY KEY (name)) ENGINE = InnoDB;#初始化记录INSERT INTO sequence (name, current_value, increment) VALUES ('GLOBAL', 10000, 100); 创建存储过程 12345678910111213141516171819202122232425262728293031323334# 获取当前sequence值DROP FUNCTION IF EXISTS mycat_seq_currval;DELIMITER $CREATE FUNCTION mycat_seq_currval(seq_name VARCHAR(50)) RETURNS VARCHAR(64) CHARSET utf8DETERMINISTICBEGINDECLARE retval VARCHAR(64);SET retval = \"-999999999,NULL\";SELECT concat(CAST(current_value AS CHAR), \",\", CAST(increment AS CHAR)) INTO retval FROM sequence WHERE name = seq_name;RETURN retval;END $DELIMITER ;# 设置sequence值DROP FUNCTION IF EXISTS mycat_seq_setval;DELIMITER $CREATE FUNCTION mycat_seq_setval(seq_name VARCHAR(50),value INTEGER) RETURNS VARCHAR(64) CHARSET utf8DETERMINISTICBEGINUPDATE sequence SET current_value = value WHERE name = seq_name;RETURN mycat_seq_currval(seq_name);END $DELIMITER ;# 获取下一个sequence值DROP FUNCTION IF EXISTS mycat_seq_nextval;DELIMITER $CREATE FUNCTION mycat_seq_nextval(seq_name VARCHAR(50)) RETURNS VARCHAR(64) CHARSET utf8DETERMINISTICBEGINUPDATE sequence SET current_value = current_value + increment WHERE name = seq_name;RETURN mycat_seq_currval(seq_name);END $DELIMITER ; sequence_db_conf.properties 配置 指定 sequence 表所在的节点信息 12345#sequence stored in datanodeGLOBAL=dn1COMPANY=dn1CUSTOMER=dn1ORDERS=dn1 本地时间戳方式ID = 64 位二进制[42位(毫秒)+5位(机器ID)+5位(业务编码)+12位(重复累加)]，换算成十进制 18 位数的 long 类型，每毫秒可以并发 12 位二进制的累加 配置 server.xml1&lt;property name=\"sequenceHandlerType\"&gt;2&lt;/&gt; 配置 sequence_time_conf.properties WORKID=0~31：可取 0 ~ 31 的任意整数 DATAACENTERID=0~31：可取 0 ~ 31 的任意整数其他方式 使用 catlet 注解 使用 zookeeper 实现自增长主键利用 MySQL 数据库的自增主键功能，创建一个只有自增 ID 的表，在 table 标签中将 autoIncrement 置为 true，在 sequence_db_conf.properties 配置改数据表所在节点，在数据库的 sequence 表中增加该表的 sequence 记录 其他配置缓存配置文件1234567#used for mycat cache service conffactory.encache=io.mycat.cache.impl.EnchachePooFactory#key is pool name ,value is type,max size, expire secondspool.SQLRouteCache=encache,10000,1800pool.ER_SQL2PARENTID=encache,1000,1800layedpool.TableID2DataNodeCache=encache,10000,18000layedpool.TableID2DataNodeCache.TESTDB_ORDERS=50000,18000 factory.encache 指定缓存的实现类，不同的缓存实现类对应不同的缓存框架，后面的指定缓存的框架、缓存大小、过期时间 日志文件配置日志的输出路径以及日志级别","tags":[]},{"title":"设计/架构/亿级流量高可用高并发/负载均衡与反向代理","date":"2020-02-22T07:55:38.734Z","path":"2020/02/22/设计/架构/亿级流量高可用高并发/负载均衡与反向代理/","text":"负载均衡与反向代理 上游服务器配置：使用 upstream server 配置上游服务器配置 负载均衡算法：配置多个上游服务器时的负载均衡机制 失败重试机制：配置当超时或者上游服务器不存活时，是否需要重试其他上游服务器 服务器心跳检查：上游服务器的健康检查 / 心跳检查 upstream配置1234upstream backend &#123; server 192.168.61.1:9080 weight&#x3D;1; server 192.168.61.2:8080 weight&#x3D;1;&#125; IP 地址和端口：配置上游服务器的 IP 地址和端口 权重：weight配置权重，默认是1，权重越高分配到的请求量就越多，按照权重比例分配","tags":[]},{"title":"设计/架构/亿级流量高可用高并发/交易系统设计原则","date":"2020-02-22T07:55:35.873Z","path":"2020/02/22/设计/架构/亿级流量高可用高并发/交易系统设计原则/","text":"高并发原则 无状态：应用无状态，配置文件有状态。应用要支持水平扩展 拆分：大型系统设计时，要按照功能模块进行拆分，有以下几个维度 系统维度：按照系统功能/业务拆分 功能维度：对一个系统进行功能再拆分 读写维度：根据读写比例特征进行拆分（读服务考虑使用缓存，写入量大考虑使用分库分表） AOP维度：根据访问特征，进行AOP拆分 模块维度：按照基础或者代码模块拆分，基础模块分库分表、数据库连接池；代码结构按照三层（Web、Service、DAO） 服务化：服务独立部署，避免相互影响；进程内服务 -&gt; 单机远程服务 -&gt; 集群手动注册服务 -&gt; 自动注册和发现服务 -&gt; 服务的分组/隔离/路由 -&gt; 服务治理 消息队列：使用消息队列进行服务解耦，可做大流量缓冲，但是要考虑数据校对问题 数据异构： 数据聚合：数据异构把数据从多个数据源拿过来，在做聚合给前端 前段展示：可通过一个或少量几个请求获取数据 缓存： 浏览器缓存：可用于对实时性要求较低的数据，例如静态文件、广告等等 APP客户端缓存：防止大促瞬间流量冲刷，可以提前把一些静态文件下发缓存 CDN缓存：利用CDN节点为用户推送数据； 推送机制：内容节点变更后，推送到CDN边缘节点； 拉取机制：先访问边缘节点，当没有内容时，回源到源服务节点拿取内容并缓存到CDN边缘节点。在设计URL时要注意不要有随机数，这样每次都会穿透CDN回源到源服务器，相当于CDN没有作用。 接入层缓存： URL重写：按照规定的设计格式、顺序重写，避免随机数 一致性哈希：按照指定的参数做一致性哈希，保证相同的数据落到同一台服务器上 proxy_cache：使用内存 / SSD 级代理缓存来缓存内容 proxy_cache_lock：使用 lock 机制，将多个回源合并为一个，减少回源量 shared_dict：如果架构使用 Nginx+Lua 实现，可以考虑使用 Lua shard_dict 进行缓存，最大的好处就是reload缓存不会丢失。Nginx + Lua参考 应用层缓存：堆内缓存、堆外缓存 分布式缓存：部署分布式缓存集群 并发化：非依赖服务并发请求 高可用原则 降级 开关集中化管理：通过推送机制把开关推送到各个应用 可降级的多级读服务：服务降级为只读本地缓存、只读分布式缓存、只读默认降级数据 开关前置化：在 Nginx 层做开关，请求流量不回源到后端服务器 业务降级：当高并发流量来袭，为保证主服务，将部分同步服务改为异步，优先处理高优先级数据或特殊特征数据，合理分配进入系统的流量，最终要保持数据一致性。 限流：做好防火墙 恶意请求只访问到 cache 恶意 IP 拦截 使用 Nginx 的 limit 模块处理，避免流量超出系统峰值 切流量：某机房瘫痪切换流量到其他机房 DNS：切换机房入口 HttpDNS：客户端分配好流量入口，绕过运营商LocalDNS并实现高精准的流量调度 LVS / HsProxy：切换故障的Nginx接入层 Nginx：切换故障的应用接入层 可回滚 业务设计原则 防重设计 幂等设计 流程可定义 状态与状态机 后台系统操作可反馈 后台系统审批化 文档和注释 备份","tags":[]},{"title":"数据结构和算法/数据结构与算法之美/数据结构与算法","date":"2020-02-22T07:46:54.571Z","path":"2020/02/22/数据结构和算法/数据结构与算法之美/数据结构与算法/","text":"数据结构是为算法服务的，算法要作用在特定的数据结构上 数据结构和算法图谱","tags":[]},{"title":"数据结构和算法/数据结构与算法之美/1.复杂度分析","date":"2020-02-22T07:46:51.484Z","path":"2020/02/22/数据结构和算法/数据结构与算法之美/1.复杂度分析/","text":"复杂度分析对于算法执行的时间和空间的分析，通过分析可以衡量算法的执行效率。 事后统计法通过统计、监控得到算法执行的时间和占用的内存大小 局限性 测试结果依赖于测试环境 测试结果受数据规模的影响比较大大 O 复杂度表示法代码的执行时间 T(n) 与每行代码的执行次数 n 成正比 1T(n) &#x3D; O(f(n)) 大 O 时间复杂度表示法 表示代码执行时间随数据规模增长的变化趋势，也叫渐进时间复杂度，简称时间复杂度。 例如：假设每行代码的执行时间是 unitTime 且每行代码的执行时间相同，则下面代码的执行总时间是 $(2n + 2) * unitTime$ 即 $T(n) = O(2n + 2)$，当 n 趋近于无穷大时，则用量级表示即可：$T(n) = O(n)$，如果是双层循环则是：$T(n) = O(n^2)$ 12345678int cal(int n) &#123; int sum = 0; int i = 1; for (; i &lt;= n; ++i) &#123; sum = sum + i; &#125; return sum;&#125; 时间复杂度分析分析方法 关注循环执行次数最多的一段代码 加法法则：总复杂的等于量级最大的那段代码的复杂度 乘法法则：嵌套代码的复杂度等于嵌套内外代码复杂度的乘机常见复杂度量级 常量阶 $O(1)$ 对数阶 $O(logn)$ 线性阶 $O(n)$ 线性对数阶 $O(nlogn)$ k 次方阶 $O(n^2)$ $O(n^3)$…$O(n^k)$ 指数阶 $O(2^n)$ 阶乘阶 $O(n!)$ 非多项式量级：$O(2^n)$ 和 $O(n!)$，其他的是多项式量级，非多项式量级的算法问题叫做 NP (Non-Deterministic Polynomial)非确定多项式问题 空间复杂度分析空间复杂度是算法的存储空间与数据规模之间的增长关系，也叫做渐进空间复杂度 复杂度分析补充最好情况时间复杂度在理想的情况下执行代码的时间复杂度；例如下面代码的最好情况时间复杂度是：$O(1)$ 123456789101112// n 表示数组 array 的长度int find(int[] array, int n, int x) &#123; int i = 0; int pos = -1; for (; i &lt; n; ++i) &#123; if (array[i] == x) &#123; pos = i; break; &#125; &#125; return pos;&#125; 最坏情况时间复杂度在最糟糕的情况下执行代码的时间复杂度；例如上面代码的最坏情况时间复杂度是：$O(n)$ 平均情况时间复杂度将每种情况出现的概率考虑进去，例如上面的代码：要查找的数据出现在数组中和不出现在数组中的概率是$1/2$，出现在 0～n-1 这 n 个位置的概率是 $1/n$，则要查找的数据可能出现在数组中的概率是：$1/2n$，将每种情况发生的情况考虑进去就是，去掉系数常量就是 $O(n)$$$1\\frac{1}{2n} + 2\\frac{1}{2n}+…+n\\frac{1}{2n}+n\\frac{1}{2}=\\frac{n(n+1)}{4n}+\\frac{n}{2}=\\frac{3n+1}{4}$$ 均摊时间复杂度对一个数据结构的一组操作中，大部分情况下耗时都很低，个别情况比较高，并且这些操作存在前后连贯的时序关系，可以将这一组操作放在一块儿分析，将高耗时的分摊到其他情况中。","tags":[]},{"title":"数据结构和算法/数据结构与算法之美/2.数组","date":"2020-02-22T07:46:49.616Z","path":"2020/02/22/数据结构和算法/数据结构与算法之美/2.数组/","text":"数组数组：一种线性表数据结构，用一组连续的内存空间，来存储一组具有相同类型的数据 如何实现数组随机访问线性表特性数据像排成一条线，每个线性表上的数据最多只有两个方向；数组、链表、队列、栈都是线性表。相对立就是非线性表，比如二叉树、堆、图等 连续的存储空间和相同的数据类型数组的连续存储使得地址空间可以根据第几个元素和每个元素的空间大小算出地址，从而可以实现随机访问，但是随即访问也使得删除和插入变得低效，需要迁移很多数据。寻址公式： 1a[i] = base_address + i * data_type_size 对于一个 m*n 的二维数组，寻址公式： 1a[i][j] = base_address + (i * n + j) * data_type_size 数组的插入删除插入在一个长度为 n 的数组中在 k 位置插入一个元素，则需要把后面 n - k 个元素向后移一位，如果刚好是最后一个元素，则时间复杂度是 $O(1)$，如果是第一个元素则是 $O(n)$，平均时间复杂度是 $(1+2+3+…+n) / n = O(n)$。 当插入的数列本身是无序的时候，可以把插入的位置的元素直接放到数组的最后，避免大规模移动数据 删除删除的场景类似于出入，如果数组无序，可以把最后一个元素放到要删除的位置，也可以避免大规模移动数据。 也可以利用 JVM 标记清除的思想，将元素标记为删除，当空间不足或者达到一定量时，统一做删除操作 容器和数组数组需要提前划分好连续的地址空间大小，无法动态扩容；容器可以支持动态扩容 数组越界问题访问超过划分的地址空间就会出现异常。但对于一些语言和编译器有可能会出现无限循环的情况。例如这段 C 语言，在有的编译器下可能出现循环访问的情况。 123456789int main(int argc, char* argv[])&#123; int i = 0; int arr[3] = &#123;0&#125;; for(; i&lt;=3; i++)&#123; arr[i] = 0; printf(\"hello world\\n\"); &#125; return 0;&#125; C 语言中除受限内存空间所有内存可以自由访问。数组大小为 3，a[0]、a[1]、a[2]，当数组大于3时才结束循环，此时会造成访问越界，但这个地址刚好是变量 i 的地址，就会导致无限循环。这里的循环问题也是特例，这个还和编译器分配内存和字节对齐相关。 数组下标从 0 开始数组下标为 0 时寻址公式： 1a[i] = base_address + i * data_type_size 下标为 1 时寻址公式： 1a[i] = base_address + (i - 1) * data_type_size 对 CPU 来说需要多做一次减法操作，在以前机器性能不高时可能有一定作用。现在来看应该作用不大。保留下来应该也有历史原因，现在有些语言数组不一定从 0 开始，甚至可以有负数，例如 python。","tags":[]},{"title":"数据结构和算法/数据结构与算法之美/3.链表","date":"2020-02-22T07:46:46.498Z","path":"2020/02/22/数据结构和算法/数据结构与算法之美/3.链表/","text":"链表链表介绍 线性表数据结构 内存空间不连续，将分散的内存块串联起来，进行数据存储 链表的每个数据节点，不仅存储数据还要存储下一个节点的地址链表特点 由于链表的数据结构，插入、删除效率高(改变指针指向即可)，时间复杂度 $O(1)$，但是随机访问的速度较慢，需要从头遍历链表。这个和数组相反 由于链表的节点不仅存储数据还要存储下一节点的指针，所以存储空间相较于数组消耗较大单链表、循环链表、双向链表、双向循环链表单链表 每个节点存储数据和下一个节点的指针，即后继指针 第一个节点是头结点，记录链表的基地址；最后一个是尾节点，指向 NULL 单链表的的插入和删除的时间复杂度是 $O(1)$，查询的时间复杂度是 $O(n)$。循环链表 尾节点的后继节点是头结点 比较适合于处理环形数据结构的问题，例如约瑟夫环问题双向链表 每个节点有后继指针和前驱指针；头结点的前驱指针是 NULL，尾节点的后继指针是 NULL 插入、删除操作比单链表效率更高 $O(1)$ 级别。以删除操作为例，删除操作分为 2 种情况：给定数据值删除对应节点和给定节点地址删除节点。对于前一种情况，单链表和双向链表都需要从头到尾进行遍历从而找到对应节点进行删除，时间复杂度为 $O(n)$ 。对于第二种情况，要进行删除操作必须找到前驱节点，单链表需要从头到尾进行遍历直到 p-&gt;next = q，时间复杂度为 $O(n)$，而双向链表可以直接找到前驱节点，时间复杂度为 $O(1)$。双向循环链表 首节点的前驱指针指向尾节点，尾节点的后继指针指向首节点指针和引用 指针和引用是同一个概念，都是存储所指对象的内存地址 将某个变量赋值给指针就是将变量的内存地址赋值给指针；对应的就是指针存储的就是变量的内存地址，通过这个指针就可以找到这个变量指针丢失、内存泄漏 链表操作时，要注意指针的交换顺序，避免出现指针丢失的情况。例如，要在当前节点 p 插入节点 x，示例1的操作会导致 x-&gt;next = x，造成整个链表断裂；只需将两个语句顺序交换即可。123456// 示例 1p-&gt;next = x;x-&gt;next = p-&gt;next;// 示例 2x-&gt;next = p-&gt;next;p-&gt;next = x; 在没有自动管理内存的编程语言，再删除节点后要手动释放内存空间，避免造成内存泄漏利用哨兵实现插入、删除 针对链表的插入、删除，插入时需要对链表的第一个节点和最后一个节点进行特殊判断； 引入哨兵，“哨兵”节点不存储数据，无论链表是否为空，head指针都会指向它，作为链表的头结点始终存在。这样，插入第一个节点和插入其他节点，删除最后一个节点和删除其他节点都可以统一为相同的代码实现逻辑了。","tags":[]},{"title":"数据结构和算法/数据结构与算法之美/4.栈","date":"2020-02-22T07:46:45.089Z","path":"2020/02/22/数据结构和算法/数据结构与算法之美/4.栈/","text":"栈先进后出，后进先出。栈是一种操作受限的线性表，只允许在一端插入和删除。用数组实现的栈叫做顺序栈，用链表实现的栈叫做链式栈。 复杂度分析 空间复杂度：栈在操作过程中，存储空间是提前申请好的，空间复杂度是 $O(1)$ 时间复杂度：栈的操作只涉及个别元素的操作，所以时间复杂度是 $O(1)$顺序栈、链式栈扩容 顺序栈：顺序栈在栈满之后如果需要扩容需要重新申请一个更大的数组，将数据迁移过去 链式栈：栈支持动态扩容，但是需要存储后继节点的地址指针，内存消耗较多栈的应用函数调用、表达式求值、括号匹配利用栈实现浏览器的前进后退需要使用两个栈：前进栈 X，后退栈 Y，将一次浏览的页面（a、b、c）按顺序压入前进栈 X，在 c 页面点后退则将 c从 X 出栈，然后放入后退栈 Y 中；再点后退的话同理，将 b 放入 Y 中，此时点前进 则把 b 从Y中出栈，放入 X 中，需要注意的是前进栈 X 入栈时将入栈的元素和 Y 栈的栈顶元素比较，如果相同则 Y 栈顶出栈，不相同则将 Y 栈清空。","tags":[]},{"title":"数据结构和算法/数据结构与算法之美/5.队列","date":"2020-02-22T07:46:43.368Z","path":"2020/02/22/数据结构和算法/数据结构与算法之美/5.队列/","text":"队列什么是队列 先进先出，也是一种操作受限的线性表数据结构。用数组实现的队列叫做顺序队列，用链表实现的队列叫做链式队列 队列支持队头入队，队尾出队 队列需要有两个指针 head 指向头指针，tail 指向队尾指针队列实现顺序队列顺序队列当入队、出队时 head 指针和 tail都向后移。顺序队列当 tail 指针到达数组尾部，数组 索引 [0 - head] 之间的空间就会浪费掉，可以利用数据迁移的想法将在入队时将数据整体往前迁移。这样出队时间复杂度是 ${O(1)}$ ，入队时需要数据搬移，最好的情况就是 head 指针在数组索引 0 的位置，不需要迁移，否则假如队列有 n 个元素，则需要迁移 n 次。则平均时间复杂度$(1+2+3+…+n) / n = O(n)$。链式队列链式队列也需要两个指针，head 指针和 tail 指针。链式队列的出队、入队时间复杂度都是${O(1)}$。12345// 入队操作tail.next = newNode;tail = tail.next;// 出队操作head = head.next; 循环队列用数组实现的队列当出队时会造成空间浪费，所以需要数据搬移，利用循环队列来解决这个问题。 关键的地方是如何判断堆满和队空的条件： 12head = (head + 1) % n;tail = (tail + 1) % n; 阻塞队列队列为空时出队操作会被阻塞，此时还无数据可取；队列满时入队被阻塞直到队列中有空闲位置。类似于 生产者-消费者 模型。当多个线程同时操作队列时就会出现线程安全问题。 并发队列线程安全的队列叫做并发队列，需要在出队、入队操作上加锁。基于数组的循环队列利用 CAS 原子操作可以实现高效的并发队列。","tags":[]},{"title":"数据结构和算法/数据结构与算法之美/6.递归","date":"2020-02-22T07:46:41.146Z","path":"2020/02/22/数据结构和算法/数据结构与算法之美/6.递归/","text":"递归递归简介把规模大的问题转化为规模小的相似子问题来处理，子问题要有明显的结束条件。所有的所有的递归问题都可以用递归公示来表示。 递归需要满足的条件1、一个问题的解可以拆分为几个子问题的解2、这个问题与分解之后的子问题，除了数据规模不同，求解思路完全相同3、存在递归终止条件 递归优缺点优点：代码表达能力强，简介缺点：1、空间复杂度高2、堆栈溢出风险，可记录堆栈深度，达到一定时抛出异常3、存在重复计算，利用散列表揭露某个字问题的解4、过多的函数计算会耗时较多 如何写递归代码写出递推公式，找出终止条件。","tags":[]},{"title":"数据结构和算法/数据结构与算法之美/7.排序","date":"2020-02-22T07:46:16.878Z","path":"2020/02/22/数据结构和算法/数据结构与算法之美/7.排序/","text":"排序算法 算法 时间复杂度 是否基于比较 冒泡、插入、选择 $O(n^2)$ 是 快排、归并 $O(nlogn)$ 是 桶、计数、基数 $O(n)$ 否 ### 排序算法分析 #### 算法的执行效率 - 最好、最坏、平均情况时间复杂度比较：根据数据的特点(无序、基本有序等)选择合适的算法 - 时间复杂度的系数、常数、低阶：数据规模很大时一般忽略了这几个量，但是当同一阶的算法比较时也需要把这几个量考虑进来 - 比较次数和交换次数：基于比较的算法有两个操作：比较和移动，在分析效率时也需要考虑。 #### 算法的内存消耗 - 数据量较大时，执行时也需要考虑内存的消耗，原地排序特指空间复杂度是 $O(1)$ 的排序算法。 #### 排序算法的稳定性 - 稳定性：排序队列中存在值相同的元素，经过排序之后想等元素之间原有的先后顺序不变。（这个特点主要实际项目中一组对象的排序时比较重要） ### 冒泡排序 比较相邻的两个元素，并且移动元素让其满足大小关系要求。经过一次冒泡至少有一个元素移动到他应该在的位置。 #### 复杂度分析 稳定性：冒泡排序中相等的两个元素不做交换时是稳定的排序算法 空间复杂度：只需要常量级的临时空间，空间复杂度为 $O(1)$，是一个原地排序算法 时间复杂度： - 最好的情况下只需要一次冒泡：时间复杂度为：$O(1)$ - 最坏的情况下是需要 n 次，时间复杂度为：$O(n^2)$ - 平均时间复杂度：$O(n^2)$ #### 有序度 数组中具有有序关系的元素对的个数，比如 [2,4,3,1,5,6] 这组数据的有序度就是 11，分别是 [2,4][2,3][2,5][2,6][4,5][4,6][3,5][3,6][1,5][1,6][5,6]。对于一个倒序数组，比如 [6,5,4,3,2,1]，有序度是 0；对于一个完全有序的数组，比如 [1,2,3,4,5,6]，有序度为 $n(n-1)/2$，完全有序的情况称为满有序度。即有 *逆序度=满有序度-有序度**。 排序过程，就是有序度增加，逆序度减少的过程，最后达到满有序度。 ### 插入排序 通过比较将无序队列中的元素不断的插入到有序队列中。插入时要保证有序队列是一直有序的。 #### 复杂度分析 稳定性：出现相等的元素可以保持插入到前面元素的后面，即是稳定的。 空间复杂度：插入排序不需要额外的空间，复杂度为 $O(1)$，是原地排序。 时间复杂度： - 最好的情况下只需要一次冒泡：时间复杂度为：$O(1)$ - 最坏的情况下是需要 n 次，时间复杂度为 $O(n^2)$。 - 平均时间复杂度为 $O(n^2)$ ### 选择排序 区分有序和无序队列，每次从无序队列中找出最小元素发到有序队列的末尾。 #### 复杂度分析 稳定性：不能保证稳定性。例如：5，8，5，2，9第一次找到最小元素 2，与第一个 5 交换位置，那第一个 5 和中间的 5 顺序就变了。 空间复杂度：插入排序不需要额外的空间，复杂度为 $O(1)$，是原地排序。 时间复杂度： - 最好的情况下只需要一次冒泡：时间复杂度为：$O(1)$ - 最坏的情况下是需要 n 次，时间复杂度为 $O(n^2)$。 - 平均时间复杂度为 $O(n^2)$ 归并排序归并排序采用分治思想，将整体划分为小部分，最后归并在一起达到整体有序。 实现分治和递归比较相像，分治是一种解决问题的思想，递归是一种编程技巧。归并排序的递归公式： 1234&#x2F;&#x2F; 递推公式merge_sort(p...r) &#x3D; merge(merge_sort(p...q), merge_sort(q + 1, r))&#x2F;&#x2F; 终止条件，当子数组不可再分解p &gt;&#x3D; r 伪代码实现： 12345678910111213141516// A 数组，n 数组大小 merge_sort(A, n) &#123; merge_sort_c(A, 0, n-1);&#125;//递归函数merge_sort_c(A, p, r) &#123; //终止条件 if p &gt;= r then return //取p-r中间位置q q = (p + r) / 2 merge_sort_c(A, p, q) merge_sort_c(A, q + 1, r) //将有序数组A[p, q], A[q + 1, r]合并到A[p, r]中 merge(A[p, r], A[p, q], A[q + 1, r])&#125; merge 函数将有序子数组合并，伪代码： 123456789101112131415161718192021merge(A[p, r], A[p, q], A[q + 1, r]) &#123; var i = p, j = q + 1, k = 0 //申请一个和 A[p, r]一样大的数组 var temp = new Array[0, r - p] while(i &lt;= q AND j &lt;= r) do &#123; if A[i] &lt; A[j] &#123; temp[k++] = A[i++] &#125; else &#123; temp[k++] = A[j++] &#125; &#125; //判断哪个子数组中只有剩余元素将其拷贝到temp var start = i, end = q if j &lt;= r then start = j, end = r //将剩余数组拷贝到临时数组中 while start &lt;= end do &#123; temp[k++] = A[start++] &#125; //将临时数组拷贝到 A 数组中 A = temp&#125; 复杂度分析稳定性归并排序是否稳定主要是最后的 merge 函数，合并的时候如果遇到值相等的元素，保持 A[p, q] 的元素在前即可保证稳定性。 时间复杂度归并排序采用了分治的思想，时间复杂度是求解各个子问题的和加上最后归并的时间。 12&#x2F;&#x2F;将问题 a 分解为 b 和 c，K是最后合并b和c的时间 T(a) &#x3D; T(b) + T(c) + K 归并排序的复杂度公示： 1234567891011T(1) &#x3D; C;T(n) &#x3D; 2*T(n&#x2F;2) + n;T(n) &#x3D; 2*T(n&#x2F;2) + n &#x3D; 2*(2*T(n&#x2F;4) + n&#x2F;2) + n &#x3D; 4*T(n&#x2F;4) + 2*n &#x3D; 4*(2*T(n&#x2F;8) + n&#x2F;4) + 2*n &#x3D; 8*T(n&#x2F;8) + 3*n &#x3D; 8*(2*T(n&#x2F;16) + n&#x2F;8) + 3*n &#x3D; 16*T(n&#x2F;16) + 4*n ...... &#x3D; 2^k * T(n&#x2F;2^k) + k * n ...... 达到终止条件即 $T(n/2^k) = T(1)$ 时，有 12345n&#x2F;2^k &#x3D; 1k &#x3D; nlog_2nT(n) &#x3D; 2^kT(n&#x2F;2^k) + kn &#x3D; 2^kC + kn &#x3D; nC + nlog_2n 归并排序和原数组的有序程度无关，时间复杂度都是 $nlog_2n$ 空间复杂度归并排序需要开辟和原数组一样大的空间辅助排序，空间复杂度时 $O(n)$ 快速排序快速排序也利用分治思想，对 A[p, r] 进行排序，选择 p-r 之间任意一个数据 pivot 作为分区点，遍历 p-r 的数据，将小于 pivot 的数据移到左边，大于 pivot 的数据放到右边，pivot 的位置为 q，原数组分为 A[p, q-1]、A[q] 和 A[q+1, r]，如此区分直到区间缩小为1，则所有数据都有序。 实现1234&#x2F;&#x2F; 递推公式：quick_sort(p, r) &#x3D; quick_sort(p, q-1) + quick_sort(q+1, r)&#x2F;&#x2F;终止条件：p &gt;&#x3D; r 123456789101112131415161718192021222324&#x2F;&#x2F; 快速排序，A是数组，n表示数组的大小quick_sort(A, n) &#123; quick_sort_c(A, 0, n-1)&#125;&#x2F;&#x2F; 快速排序递归函数，p,r为下标quick_sort_c(A, p, r) &#123; if p &gt;&#x3D; r then return q &#x3D; partition(A, p, r) &#x2F;&#x2F; 获取分区点 quick_sort_c(A, p, q-1) quick_sort_c(A, q+1, r)&#125;partition(A, p, r) &#123; pivot :&#x3D; A[r] i :&#x3D; p for j :&#x3D; p to r-1 do &#123; if A[j] &lt; pivot &#123; swap A[i] with A[j] i :&#x3D; i+1 &#125; &#125; swap A[i] with A[r] return i 这里分区函数的处理类似于插入排序，将 A[p, r-1] 分为两部分，A[p, i-1] 的元素都小于 pivot，暂叫做已处理区间，每次从未处理区间 A[i, r-1] 取出一个元素和 pivot 比较，如果小于则交换到未处理区间 复杂度分析稳定性由于分区函数存在元素交换，所以快速排序是不稳定的算法 时间复杂度正常情况下，如果分区合理则快速排序的时间复杂度与归并排序一样是 $O(nlog_2n)$ 12T(1) &#x3D; C； n&#x3D;1时，只需要常量级的执行时间，所以表示为C。T(n) &#x3D; 2*T(n&#x2F;2) + n； n&gt;1 如果选择的最后一个分区元素是最大的，则分区就不均衡，会退化成 $O(n^2)$ 空间复杂度在排序过程中，分区函数的实现在原数组中，原地处理。空间复杂度是 $O(1)$ 归并排序与快速排序区别 归并排序：先递归调用然后合并处理。从下往上先处理子问题。 快速排序：先分区在递归调用，从上往下处理，到最后已经有序。选择分区点 三位取中法：从区间的首、中、尾分别选取三个元素，选取三个元素的中位元素位置作为分区点 随机法：随机选取元素作为分区点，降低最大元素成为分区点的概率桶排序先对原有的值域进行划分，将元素区分到每个桶中，然后每个桶各自排序，最后将桶合并。桶排序是针对一些有特征的数据集效果较好。实现 值域划分，通过规则将每个元素映射到对应的桶，确定桶的个数。需要根据数据集的特征来确定映射规则。例如：123&#x2F;&#x2F; f(x) 是元素 x 所在桶的编号；length 是原数据集的大小f(x) &#x3D; (x - min) &#x2F; lengthbucketNum &#x3D; (max - min) &#x2F; length + 1 排序算法，每个桶的排序算法可以自定。复杂度分析时间复杂度数据集有 n 个元素，桶的个数为 m，如果是均匀的划分，每个桶内的元素个数为 $k = n/m$。桶内使用快速排序来实现，每个桶的时间复杂度是 $O(klogk)$，m 个桶的时间为 $O(mklogk)$，$k = n/m$ 则有 $O(nlog(n/m))$，当桶的个数接近 n 时，则时间复杂度接近为 $O(n)$。空间复杂度需要申请 m 个桶空间，整体的空间复杂度是 $O(m + n)$稳定性桶排序的稳定性与每个桶内所采用的排序算法相关计数排序通过辅助数组，遍历原集合将每个元素标记在对应的位置，遇到相同的元素则对应的位置计数器自增。实现 计算辅助数组的大小1size &#x3D; max + 1 遍历待排序集合，将每个元素出现的次数记录到辅助数组中 计算每个元素的最终位置，从后往前遍历保证稳定性 例如，数组 A[2, 4, 5, 3, 4, 1]： 辅助数组大小为 6，遍历后得到 C[0, 1, 1, 1, 2, 1] 计算小于等于每个元素的个数，可以通过 C 数组得到，即为 C[i - 1] + C[i]，得到 C[0, 1, 2, 3, 5, 6] 临时数组 R 存放排序后的元素，从后往前遍历数组 A，A[5] = 1，对应的小于等于 1 的元素 C[1] 是 1，则元素 1 应该排在 R 的第一个位置即 R[0] = 1，同时 C[1] 的值自减 1；A[4] = 4，小于等于 4 的元素 C[4] = 5，则 A[4] 元素放在 R[4] 的位置。同时 C[4] 自减 1；……循环到数组 A 被遍历完。 最后得到的 R 数组就是数组 A 排序后的结果。复杂度分析时间复杂度原数组大小是 N，辅助数组大小是 M，原数组存在多次遍历，但是去掉系数后时间复杂度是 $O(N + M)$空间复杂度申请了一个辅助排序的数组，空间复杂度是 $O(N + M)$稳定性如果在确定了元素位置后直接输出，是不稳定的，通过上述事例中的处理则是稳定的适用范围计数排序适用在范围不大的数据集中，而且要求数据是正整数，对于负数或者小数需要将其处理为整数。基数排序将元素先按照低位排序，在按照高位排序。元素位数不够的，按照最大元素的位数在不够的元素前面补 0。实现 确定最大元素及其位数 申请一个基数数组 C[] 用于统计每一位元素出现的次数 计算元素排序后的位置，这与计数排序中类似，计算小于等于 C[i] 的个数 申请一个临时数组 R[] 存储排序后的元素复杂度分析时间复杂度如果最大的元素只有一位，则时间复杂度是 $O(n)$，最大元素的位数为 k，则基数排序的时间复杂度是 $O(k*n)$空间复杂度空间复杂度是 $O(n + k)$稳定性与计数排序类似，实现过程中的处理可以保证稳定性","tags":[]},{"title":"数据存储/缓存/CsCache设计","date":"2020-02-22T07:40:41.945Z","path":"2020/02/22/数据存储/缓存/CsCache设计/","text":"CsCache缓存实现缓存架构介绍 CsCache缓存分层图 客户端层：使用者直接通过该层与数据进行交互 缓存提供层：对缓存管理层的生命周期进行维护，负责缓存管理层的创建、保存、获取和销毁 缓存管理层：对缓存客户端的生命周期进行维护，负责客户端的创建、保存、获取以及销毁 缓存存储层：负责以什么样的形式存储数据 基本存储层：以普通的ConcurrentHashMap为存储核心，不淘汰数据 LRU存储层：以最近最少使用原则进行数据存储和缓存淘汰 Weak存储层：以弱引用为原则的数据存储和缓存淘汰机制","tags":[]},{"title":"数据存储/MySQL/MySQL并发与事务","date":"2020-01-20T08:59:40.000Z","path":"2020/01/20/数据存储/MySQL/MySQL并发与事务/","text":"MySQL并发与事务并发控制MySQL 通过锁解决并发问题，一般有两种类型共享锁、排他锁，也叫读锁、写锁。 锁粒度加锁后就会影响系统的性能，需要在锁的性能损耗和数据安全性之间寻求平衡。 表锁用户写数据时需要先获得写锁，这会阻碍该表的所有的读写操作，读锁之间互不影响。写锁可以插到锁队列的读锁前面，反之则不行。 行级锁行级锁可以最大程度的支持并发处理，但这同时增大了锁的开销，MySQL 行级锁是在存储引擎层实现，服务层没有相关设计。 事务事务的特征：原子性、一致性、隔离性、持久性，即ACID 原子性：一个事务被视为一个不可分割的最小工作单元，整个事务必须全部提交或者全部回滚 一致性：数据库从一个一致性的状态转换到另一个一致性的状态 隔离性：通常情况下，一个事务所做的修改在最终提交前对于其他事务是不可见的 持久性：一旦事务提交，其所做的修改就会永久保存到数据库中 隔离级别未提交读(READ UNCOMMITTED)：事务中的修改即使没有提交对于其他事务也是可见的，事务可以读取未提交的数据，称之为脏读。 提交读(READ COMMITTED)：一个事务开始时，只能看见已经提交的事务所做的修改。即一个事务从开始到提交所做的修改对于其他事务是不可见的。也叫不可重复读。 可重复读(REPEATABLE READ)：解决了脏读问题，保证了一个事务中多次读取同一个数据结果是一致的，但是无法解决幻读。幻读是指当一个事务在读取某个范围的记录时，另一个事务在这个范围中插入了新的记录，再次读取会产生不同的结果。可重复读是 MySQL 的默认隔离级别。 可串行化(SERIALIZABLE)：强制事务串行化，会在读取的每行数据上加锁，避免了幻读。可能会造成大量的锁超时。 隔离级别 脏读可能性 不可重复读可能性 幻读可能性 加锁读 READ UNCOMMITTED YES YES YES NO READ COMMITTED NO YES YES NO REPEATABLE READ NO NO YES NO SERIALIZABLE NO NO NO YES 死锁死锁指两个或者多个事务在同一资源上相互占用，并请求对方占用的资源，导致恶性循环。多个事务同时锁定同一资源时，也会产生死锁。 InnoDB 存储引擎可以检测死锁的循环依赖并立即返回错误，InnoDB 目前处理死锁的方式是将持有最少行级锁(写锁)的事务进行回滚。 MySQL 中的事务自动提交(AUTOCOMMIT)：MySQL 默认采用自动提交模式，SHOW VARIABLES LIKE &#39;AUTOCOMMIT&#39;使用命令查看，1 或者 ON 表示启用，0 或者 OFF 表示禁用。在执行 DDL 相关命令时会强制执行 COMMIT 提交当前活动的事务。可以通过 SET SESSION TRANSACTION ISOLATION LEVEL READ COMMITTED 设置事务的隔离级别。 在事务中混合使用存储引擎：事务是由下层存储引擎实现的，所以在同一个事务中使用多个存储引擎是不可靠的。如果混合使用了事务性和非事务性的表，在提交事务时不会有什么问题，但是回滚时非事务性表上的变更无法回退。 隐式锁定和显示锁定：InnoDB 执行的是两阶段锁定协议，在事务执行过程中随时可以锁定，只有当事务提交或者回滚之后锁才回释放，所有的锁在同一时刻释放，是隐式锁定；InnoDB 支持通过特定语句 LOCK TABLES 进行显示锁定。","tags":[]},{"title":"middle-service/search/elasticsearch/elasticsearch安装","date":"2019-02-17T07:27:41.888Z","path":"2019/02/17/middle-service/search/elasticsearch/elasticsearch安装/","text":"环境 系统：MACOS Java 1.8+ 版本 版本历史：1.x -&gt; 2.x -&gt; 5.x -&gt; 6.x 下载的是 6.5.4 安装​ 首先在官网上下载压缩包，下载地址官网，或者在命令行使用wget命令获取，下载后解压即可。 目录介绍​ bin存放相关脚本命令 ​ config启动相关配置文件 ​ lib依赖的第三方库 ​ modules模块目录 ​ plugins第三方插件 ​ logs运行后会默认产生的日志文件夹 ​ data运行后产生的数据文件夹 单实例安装​ 打开terminal，目录切换至解压后的 elasticsearch 文件夹的 bin 目录下，输入java -version检查 java 版本是否符合，要求在1.8以上。 ​ 键入sh elasticsearch后回车，启动服务，默认端口 9200，在浏览器输入localhost:9200出现以下信息，启动成功。 1234567891011121314151617&#123; \"name\" : \"Bc8O_aI\", \"cluster_name\" : \"elasticsearch\", \"cluster_uuid\" : \"xYAoAJt1QFihcu9U-TRQXg\", \"version\" : &#123; \"number\" : \"6.5.4\", \"build_flavor\" : \"default\", \"build_type\" : \"tar\", \"build_hash\" : \"d2ef93d\", \"build_date\" : \"2018-12-17T21:17:40.758843Z\", \"build_snapshot\" : false, \"lucene_version\" : \"7.5.0\", \"minimum_wire_compatibility_version\" : \"5.6.0\", \"minimum_index_compatibility_version\" : \"5.0.0\" &#125;, \"tagline\" : \"You Know, for Search\"&#125; 插件安装 Head插件","tags":[]},{"title":"java/Thread/Thread-wait-sleep","date":"2019-01-20T06:38:22.702Z","path":"2019/01/20/java/Thread/Thread-wait-sleep/","text":"Thread wait &amp; sleepThread wait线程等待（Waiting） 是线程的状态之一。通过 Thread.wait() 进入等待状态的线程会自动放弃 对象锁（Monitor），然后进入线程等待状态。当其他线程调用 notify() 或 notifyAll() ，等待线程进入可运行状态（Runnable），等待 CPU 调度。线程的一生介绍了线程状态间切换的过程。 调用 Object.wait() 前，必须已经获取了对象锁，否则将抛出 IllegalMonitorStateException。 1234567891011121314public class Demo &#123; private final Object lock = new Object(); public void badUsage() &#123; // will throw IllegalMonitorStateException lock.wait(); &#125; public void goodUsage() &#123; synchronized (lock) &#123; lock.wait(); &#125; &#125;&#125; Thread sleep处于 sleep 的线程也进入 等待 状态。与 Thread.wait() 不同是：线程不会因为 sleep 而放弃对象锁。当然，在任何情况下都可以调用 Thread.sleep() 方法，即使是未获得任何对象锁的前提下。 处于 sleep 下的线程，可能被其他线程中断（Interrupt），中断响应后将抛出 InterruptedException。何时需要线程中断中介绍了更多中断的内容。 Thread awaitwait() 方法属于 Object 类，await() 方法属于 Condition 类。 两者都是需要在获取锁的前提下调用，调用成功后放弃锁。前者获取对象锁，后者获取显式锁（Java 中 Lock 的实现类）。 Object.notify() 随机唤醒一个等待线程，Condition.signal() 唤醒指定的等待线程。这是使用上最大的不同。 扩展阅读线程的一生 何时需要线程中断 Java并发编程：线程间协作的两种方式：wait、notify、notifyAll和Condition - 海子 版权声明本作品采用知识共享署名-非商业性使用 4.0 国际许可协议进行许可。 感谢支持！","tags":[]},{"title":"java/Thread/线程的一生","date":"2019-01-20T06:38:22.701Z","path":"2019/01/20/java/Thread/线程的一生/","text":"线程的一生线程是 CPU 调度的基本单位。Java 中线程状态分为 6 种： New：创建状态 Runnable：可运行状态 Waiting：等待状态 Timed Waiting：限时等待状态 Blocking：阻塞状态 Terminated：结束状态 Thread.State 中介绍了各个状态的含义。 Runable创建（new） 后的线程对象，调用 Thread.start() 方法进入 可运行（Runnable） 状态。 可运行 线程并没有立即执行，而是分为了两步：ready to run 和 running。因为线程进入可运行状态后仍然需要等待某些资源，最常见的是等待 CPU 调度资源或 IO 资源。 线程执行结束后，进入了 结束（Terminated） 状态。 Waiting线程会以 3 种不同方式进入 等待（Waiting） 状态，然后以各自的方式被唤醒。唤醒后的线程并没有直接进入 可运行 状态，而是参与了锁竞争。下面详细的介绍了这个过程。 任何处于等待状态下的线程，均允许响应中断。但是，仍然需要参与锁竞争，获得锁权限后才会抛出 InterruptedException。 Object.wait()Object.wait() 调用后，线程需要先放弃对象锁（若事先未获得锁，则抛出 IllegalMonitorStateException），然后被推进 线程等待队列（Thread Waiting Set），等待其他线程的唤醒。 其他线程调用 Object.notify() 会 随机 唤醒等待队列中的一个线程。为了公平性，该线程并没有直接进入可运行状态，而是重新开始了锁竞争，成功获取锁权限后才进入可运行状态。否则，线程会因为竞争锁失败而进入 阻塞（Blocking），一直到获取锁权限。 Object.notifyAll() 会唤醒等待队列中的所有线程，后续过程与 Object.wait() 相同。 特别注意： 等待线程仍然可以响应中断，但是需要竞争到锁权限后才会抛出 InterruptedException 等待线程有可能（概率尽管很小）出现 意外唤醒（Surprise Wakeup），因此通常在循环中调用 wait() 方法 1234567private final Object object = new Object();....synchronized (object) &#123; while (condition) &#123; object.wait(); &#125;&#125; Thread.join()Thread.join() 方法是为了等待某一个线程进入 终止（Terminated） 状态，当线程执行完毕，等待结束。 1234Thread t = new Thread();t.start();// waiting for t finished.t.join(); Thread.join() 的底层原理是基于 Object.wait() 实现的，通过循环判断线程是否存活来决定是否继续等待。 1234567public final synchronized void join(long millis) &#123; ... while (isAlive()) &#123; wait(delay); &#125; ...&#125; LockSupport.park()LockSupport 是 Java 中用于支持 线程阻塞原语（Thread Blocking Primitives，又称 PV 原语） 的基础工具类，位于 java.util.concurrent.locks 包下。Lock 与 Condition 都是基于 LockSupport 实现的。 12Lock lock = new ReentrantLock();Condition cond = lock.newCondition(); PV 原语用于空闲资源申请和释放，P 操作用于申请一个空闲资源，V 操作用于释放空闲资源，操作系统的线程管理 中介绍了 PV 原语的作用和原理。 park 方法等同于 P 操作，unpark 等同于 V 操作。调用 park 方法无法获取空闲资源时，线程会进入等待状态，直到其他线程调用 unpark 方法释放资源。 特别注意的是，park 方法进入的等待的线程，有可能被意外唤醒（与 Object.wait() 中的意外唤醒相同），为了安全起见，通常都会在循环中调用。 123while (condition) &#123; LockSupport.park(this);&#125; Timed Waiting限时等待（Timed Waiting） 与 等待（Waiting） 状态相似，但是它可以在超时后在没有外界的影响下自我唤醒。 下面是关于各个限时等待方法的参数对于边界值的处理方式： 等于 0 小于 0 Thread.sleep(millis) 立即唤醒 IllegalArgumentException Thread.join(millis) 与 jion() 相同 IllegalArgumentException Object.wait(timeout) 与 wait() 相同 IllegalArgumentException LockSupport.parkNanos 无任何操作，直接返回 无任何操作，直接返回 LockSupport.parkUtil 无任何操作，直接返回 无任何操作，直接返回 注：使用 LockSupport.parkNanos 和 LockSupport.parkUtil 时 必须 保证等待参数大于 0，否则方法无效。 Terminated执行完任务的线程，会进入 结束（Terminated） 状态。 以下是 Java 线程状态变化流程图。 thread_lifecycle Tick, Tick线程状态是 Java 多线程“游戏”中的基本规则。本篇重点讲解的线程等待与唤醒，恰恰是这场游戏中最复杂、最关键的一环。Java 中诸多熟知的多线程工具，例如：可重入锁、Condition、Thread.join 等都是基于线程等待实现的。 扩展阅读操作系统的线程管理 Life Cycle of a Thread in Java | Baeldung 版权声明本作品采用知识共享署名-非商业性使用 4.0 国际许可协议进行许可。 感谢支持！","tags":[]},{"title":"java/jvm/Java的引用与回收","date":"2019-01-20T06:38:22.701Z","path":"2019/01/20/java/jvm/Java的引用与回收/","text":"Java 的引用与回收Java 中引用类型中主要包含了：强引用、软引用、弱引用、虚引用（本篇不涉及）。 强引用：通过 new 创建的引用，被强引用指向的对象，不会被 gc 回收。 1Object ref = new Object(); 软引用：通过 SoftReference 包裹的引用，内存不足时，如果没有强引用指向它则被 gc 回收。 1SoftReference weakRef = new SoftReference(new Object); 弱引用：通过 WeakReference 包裹的引用，下一次 gc 时，如果没有强引用指向它则被 gc 回收。 1WeakReference weakRef = new WeakReference(new Object); 软引用 &amp; 弱引用的使用软引用一般用于可伸缩式缓存，即缓存本身的大小不固定，可随着存储空间的增加而增加。因为软引用只有在内存不足时，才会被 gc 回收。 基于弱引用的回收特性，最常见的一种用法是 WeakHashMap。弱引用还可以解决 Lapsed listener problem。 当出现“内存坏账”的时候，它可以解决“坏账”对象的回收问题。例如，使用 ThreadLocal 容器时，可以将 WeakReference 作为元素，这样可以不用考虑被引用对象的回收问题。 何时被回收当使用软引用或弱引用的时候，要明白创建了 2 个对象：引用对象（Reference）和被引用对象（Referent）。被引用对象根据约定会被 gc 回收。但是由于引用对象是强引用，不会被 gc 自动回收。 12345678// ref1 and object AReference ref1 = new WeakReference(new Object());// ref2 and object BReference ref2 = new SoftReference(new Object());// release refs by handleif (ref1.get() == null) ref1 = null;if (ref2.get() == null) ref2 = null; 其中，gc 会在约定下回收 A、B 两个对象，ref1 和 ref2 不会被 gc 回收。 如果不希望被引用对象 A 被 gc 回收，需要使用强引用 N 重新指向对象，让对象 A 处于可达状态。 123// ref1 and object AReference ref1 = new WeakReference(new Object());Object N = ref1.get(); 可达性是 JVM 在内存回收时，判断对象是否可以被回收的标准，更多关于内存回收的内容在 JVM 内存回收 ReferenceQueue引用队列（ReferenceQueue） 可以和 Reference 配合使用。当 gc 会收了 Referent 后，会将 Reference 放入队列中，以此通知用户 Referent 已经被回收。 12345ReferenceQueue&lt;Object&gt; refQueue = new ReferenceQueue&lt;&gt;();WeakReference&lt;Object&gt; ref = new WeakReference&lt;&gt;(new Object(), refQueue);System.out.println(ref + \" - \" + ref.get()); // java.lang.ref.WeakReference@198e2867 - java.lang.Object@12f40c25System.gc();System.out.println(refQueue.remove()); // java.lang.ref.WeakReference@198e2867 Tick, Tick本篇重点讲解引用的回收：gc 回收的是 Referent，而不是 Reference。用于提醒读者，在后续的开发中，不要因使用了 Reference 却没有手动的清理 Reference 对象而出现内存泄漏。 扩展阅读JVM 内存回收 Weak References in Java | Baeldung Soft References in Java | Baeldung Lapsed listener problem | Wikipedia 版权声明本作品采用知识共享署名-非商业性使用 4.0 国际许可协议进行许可。 感谢支持！","tags":[]},{"title":"net-work/okhttp/OkHttpClient3架构简介","date":"2019-01-20T06:38:22.700Z","path":"2019/01/20/net-work/okhttp/OkHttpClient3架构简介/","text":"OkHttpClient3 架构简介OkHttp 旨在提供简单、稳定、高效的 HTTP Client 服务，并且支持 HTTP 2.0 以及 WebSocket。 总体架构分 6 层Protocols Layer 涵盖了 OkHttp 支持的所有“功能”，实现了复杂的协议通讯和 TSL 握手与验证。 Connection Layer 为“稳定、高效”而努力。除了单纯的实现了链路通讯，还实现了连接池（ConnectionPool）、链路复用（StreamAllocation）、失败链路黑名单（RouteDatabase）。 Cache Layer 用于缓存 Response，提高请求效率。 IO Layer 是基于 okio 的 IO 层，属于基础模块。 Interface Layer 是面向用户的接口层，目标是为用户提供简单的 API。OkHttp 将所有的需要暴露给用户的接口全部集中在了 OkHttpClient，方便用户的使用。OkHttpClient 类是外观模式的一个优秀案例。 Interceptor Layer 是其他 5 层的纽带，贯穿了整体的请求与响应流程。将这一复杂流程巧妙的划分为 5 部分，然后用拦截器分别实现各个子流程。同时，暴露给用户，方便用户接入自己的拦截器。 下图简单展示了请求与响应在 Interceptor 中的过程。 OkHttp请求与响应流程图 拦截器简介（选读） RetryAndFllowupInterceptor：创建 Stream、失败重试和重定向 BridgeInterceptor：从网络模型来看，它是传输层和应用层的桥梁，负责将用户的 Request 转换成网络可以理解的 Request；将来自网络的 Response 转换成易于用户使用的 Response CacheInterceptor：缓存获取与插入 ConnectInterceptor：开启一个连接，即从 Stream 中获取一个 RealConnection CallServerInterceptor：最后一个拦截器，用于向服务器发起原始的请求，并接收原始的响应 系统架构图 OkHttp系统架构图 Tick, Tick学习的过程应当尽可能的从宏观到微观。把握整体的脉络，走进了迷宫里才能方寸不乱，有张有弛。本篇从俯视的角度去了解 OkHttp。从它到底想解决什么问题开始，到它的整体设计如何围绕这些核心命题而展开。 “宏观到微观，问题驱动”，笔者的心得。欢迎大家一起分享自己的学习技巧，互相学习。 扩展阅读OkHttpClient3 连接池模型 版权声明本作品采用知识共享署名-非商业性使用 4.0 国际许可协议进行许可。 感谢支持！","tags":[]},{"title":"os/操作系统的线程管理","date":"2019-01-20T06:38:22.699Z","path":"2019/01/20/os/操作系统的线程管理/","text":"操作系统的线程管理 蛮荒之地，笔者正在火速开垦……","tags":[]},{"title":"java/Thread/何时需要线程中断","date":"2019-01-05T07:47:47.139Z","path":"2019/01/05/java/Thread/何时需要线程中断/","text":"何时需要线程中断 蛮荒之地，笔者正在火速开垦……","tags":[]},{"title":"net-work/okhttp/OkHttpClient3连接池模型","date":"2019-01-05T07:47:47.136Z","path":"2019/01/05/net-work/okhttp/OkHttpClient3连接池模型/","text":"OkHttpClient3 连接池模型OkHttp 的连接池与 JDBC 连接池有所不同。JDBC 的连接池往往面向单一目标服务器，而 OkHttp 中，多数情况下 HTTP 请求需要面向多台不同的服务器，因此 OkHttp 的连接池需要面向多目标。 客户端与服务器关系图 获取连接StreamAllocation#findConnections()方法用于获取一个连接（RealConnection，本文简称 cnn），cnn 可能来自线程池，创建新连接。 HTTP 2 连接复用 OkHttp 的连接复用与连接的创建和回收过程关联紧密，并且还涉及到了一个大角色——StreamAllocation HTTP 2.0 中，一个连接可以同时发送多个请求。所以 OkHttp 需要解决：如何让一个连接被多个线程使用，同时要确保线程安全，以及准确及时的回收空闲连接。 cnn 的复用是通过 StreamAllocation 实现的。StreamAllocation 可以看作是 cnn 的一个“分身”，一个 cnn 拥有众多的 StreamAllocation。用户使用连接的时，会获取 StreamAllocation，从而获取它背后真正的 cnn与服务器通讯。 StreamAllocation和Connection关系图 下面从 Java 层面展示了两者的关系。 123456789101112/* RealConnection class */public final class RealConnection extends Http2Connection.Listener implements Connection &#123; /** Current streams carried by this connection. */ public final List&lt;Reference&lt;StreamAllocation&gt;&gt; allocations = new ArrayList&lt;&gt;(); ...&#125;/* StreamAllocation class */public final class StreamAllocation &#123; private RealConnection connection; ...&#125; 一个 cnn 拥有一组 StreamAllocation 软连接集合， StreamAllocation 持有唯一一个 cnn。当用户获取一个连接时，会向特定的一个 StreamAllocation 发出申请，然后返回一个真正当连接。多个线程间可能会从不同的 StreamAllocation 获取同一个 cnn。一个线程持有一个唯一 StreamAllocation。 这里的用户是广义的，泛指连接的需求方，通常是一个线程。而并非真实的一个用户。 连接销毁 我们采用“倒叙”的方式铺开连接池的管理，从销毁讲起，后面还包括：连接的获取、连接的创建和线程安全保障 cnn 维护在 ConnectionPool#connections 中，数据结构如下： 1234public final class ConnectionPool &#123; private final Deque&lt;RealConnection&gt; connections = new ArrayDeque&lt;&gt;(); ...&#125; 完成这个清理工作的，是一个线程，它的处理方式很简单： connection pool 中没有任何空闲连接时，线程关闭 connection pool 中没有待清理的连接时，线程等待（waiting，默认等待 5 min） connection pool 中存在需要清理的连接时，执行清理任务 当有新的 cnn 加入到 connection pool 时，清理线程开启。 12345678910111213public final class ConnectionPool &#123; ... void put(RealConnection connection) &#123; assert (Thread.holdsLock(this)); if (!cleanupRunning) &#123; cleanupRunning = true; // start the cleanup thread executor.execute(cleanupRunnable); &#125; connections.add(connection); &#125; ...&#125; 连接池清理工作包括：寻找无效连接和销毁无效连接，空闲连接的判断是第一步的核心。 cnn 维护了一个 StreamAllocation 软连接集合用于对连接的使用情况进行追踪与计数。当没有任何线程使用 cnn 时，无法从集合中获取未被回收的 StreamAllocation 对象 。通过这样当方式，我们可以轻松的判断：”当前连接是否是处于空闲状态？“。 OkHttp 是允许部分空闲连接的存在的，只有超过最大空闲连接数量（maxIdleConnections）或者空闲时间过长的连接（keepAliveDurationNs），才被定义为“无效连接“，然后被清理线程销毁。 如同我们猜想的一样，销毁的最后一步，一定是将 cnn 从 connections 移除，并关闭 socket。 123456789101112131415public final class ConnectionPool &#123; long cleanup(long now) &#123; ... if (longestIdleDurationNs &gt;= this.keepAliveDurationNs || idleConnectionCount &gt; this.maxIdleConnections) &#123; // 1. remove cnn from connections connections.remove(longestIdleConnection); &#125; ... // 2. close socket // Close, ignoring any uncheck exceptions. Does nothing if socket is null. closeQuietly(longestIdleConnection.socket()); ... &#125;&#125; 结束了上面过程，简单梳理一下。清理连接的整体思路是：通过 cleanupRunnable 线程来执行清理任务，通过线程等待的方式不断的执行。StreamAllocation 软连接集合的引入，追踪了连接的被使用情况，解决了“空闲连接”定义的问题。该思路与 gc 回收算法中的“引用计数”算法大致相同。清理线程将无效的连接销毁，完成清理任务。 获取连接 &amp; 创建连接获取连接的过程包含了连接的创建。获取连接最初是由 ConnectInterceptor 拦截器发起的。拦截器模式是 OkHttp 整体流程的主干，贯穿了整体请求与响应流程，OkHttpClient3 架构简介 包含了 OkHttp 拦截器在整体架构中的地位与应用。 12345678910111213141516171819202122/* ConnectInterceptor class */public final class ConnectInterceptor implements Interceptor &#123; @Override public Response intercept(Chain chain) throws IOException &#123; ... // get a new stream HttpCodec httpCodec = streamAllocation.newStream(client, chain, doExtensiveHealthChecks); ... &#125;&#125;/* StreamAllocation class */ public HttpCodec newStream(...) &#123; ... try &#123; // find a connection RealConnection resultConnection = findHealthyConnection(connectTimeout, readTimeout, ... &#125; catch (IOException e) &#123; throw new RouteException(e); &#125; &#125; OkHttp 优先尝试从 ConnectionPool 中获取连接，获取成功后计数加 1；如果获取失败，会创建一个新的连接，并将新连接加入到 connections 中，计数加 1。 计数过程是把 StreamAllocation 对象加入到 cnn 的“影子”集合中，StreamAllocation#acquire() 完成了这项工作。 获取连接获取 cnn 就是遍历 connections ，找到一个“合适”的连接返回。OkHttp 要求 HTTP 1.x 中，一个 cnn 最多拥有 1 个 Stream，HTTP 2.0 中可以拥有多个（默认为 Integer.MAX_VALUE）。合适连接要求如下： steam 没有达到创建的上限 host 相同 计数加 1，并将合适的 cnn 成功返回给用户，完成 HTTP 请求。 这里合适连接的要求，仅停留在 HTTP 1.x，HTTP 2.0 获取连接做了一些协议上的处理，不是本篇重点，这里不详述。 创建连接当从 ConnectionPool 获取连接失败后，会选择创建新的连接。并将新的连接加入到 connections 中，计数加 1。 1234567891011121314public final class StreamAllocation &#123; private RealConnection findConnection(...) throws IOException &#123; RealConnection result = null; ... // create new RealConnection result = new RealConnection(connectionPool, selectedRoute); // add stream to connection allocations acquire(result, false); ... // Pool the connection. Internal.instance.put(connectionPool, result); ... &#125;&#125; 连接的获取与创建过程，通过创建对象，并将新创建的对象加入到连接池中，计数加 1。本质上来看是连接池的连接容器（Deque&lt;RealConnection&gt;）的 add() 和计数器计数。 线程安全 本节所介绍的线程安全仅仅是有关连接池的线程安全。更多线程模型介绍在 OkHttpClient3 线程模型。 线程安全的核心是解决 共享资源在竞争条件下的状态不确定 问题。解决过程中的关键一步在于：找出可能处于竞争条件的共享变量。 其中，被 final 修饰的基础变量、Unmofied 容器、无访问通路的私有变量和局部变量，任何情况下都处于非竞争条件。更多关于线程的介绍在 操作系统的线程管理。 竞争条件（Race Condition， 又称竞太条件）：多个进程读写共享资源，最终的结果取决于进程运行的一个精确的时序，这样的情形称之为竞争条件，例如：缓冲区的并发访问问题。 Connection Pool 的共享变量ConnectionPool 作为 OkHttpClient 的一个 final 成员，随着 OkHttpClient 的创建而创建。一个 OkHttpClient 持有唯一一个 ConnectionPool，即 ConnectionPool 相对于 OkHttpClient 是单例模式。 OkHttp 的连接池类中，成员信息如下： 12345678910111213public final class ConnectionPool &#123; private static final Executor executor = new ThreadPoolExecutor(...); private final int maxIdleConnections; private final long keepAliveDurationNs; private final Runnable cleanupRunnable = new Runnable() &#123; ... &#125;; private final Deque&lt;RealConnection&gt; connections = new ArrayDeque&lt;&gt;(); final RouteDatabase routeDatabase = new RouteDatabase(); boolean cleanupRunning; ...&#125; 其中，共享变量有三个： connections：缓存连接，在线程创建、获取、销毁时使用 routeDatabase：缓存失败的链路，监视器类 cleanupRunning：清理线程开启的标志 ConnctonsOkHttp 在任何地方都使用了对象锁来保证访问 connections 的正确性，由于 ConnetionPool 对于 OkHttpClient 而言是单例，对象锁可以解决并发冲突。 RouteDataBase一个 HTTP URL 根据 DNS 解析的结果往往会存在多条 HTTP 链路（Route），例如： 121.1.1.1 www.abc.com1.1.1.2 www.abc.com OkHttp 维护来一个失败链路的黑明单，用于记录和规避连接失败的场景，提升连接的成功率。为了保证并发安全，OkHttp 采用了监视器模式实现该类。 cleanupRunningcleanupRuning 变量只在 put() 和 cleanup() 方法中用到，这两方法均使用了对象锁来保证。 Tick, Tick本篇文档偏重于讲解 OkHttp 线程模型的“骨骼”，其中的细节没有过多的涉及。目的是了解连接池的核心——安全高效的获取连接和回收连接，这是最有“营养”的部分。关于 HTTP 的协议、代理、路由等，没有停留太久，这些不是本篇的重点。更多的介绍在 HTTP 2.0 的价值在哪里。 照猫画虎，笔者仿照 OkHttp3 连接池模型写了一个 Demo，欢迎各位大牛探讨与斧正。 扩展阅读Thread wait &amp; sleep OkHttpClient3 架构简介 Java 的引用与回收 OkHttpClient3 线程模型 操作系统的线程管理 HTTP 2.0 的价值在哪里 版权声明本作品采用知识共享署名-非商业性使用 4.0 国际许可协议进行许可。 感谢支持！","tags":[]},{"title":"net-work/netty/Linux网络IO模型简介","date":"2019-01-05T06:54:02.980Z","path":"2019/01/05/net-work/netty/Linux网络IO模型简介/","text":"Linux内核将所有外设看作文件，文件的读写操作会返回一个文件描述符fd（file descriptor） socket的响应描述符为socketfd 描述符就是一个数字指向内核中的一个结构体（文件区域、路径属性等） 阻塞IO模型 非阻塞IO模型 IO复用模型 信号驱动IO模型 异步IO 所有文件操作阻塞执行，执行完一个在执行下一个 多个文件操作可以一起处理，各自应用进程轮询对应的内核数据 通过Linux提供的select/poll处理，进程将多个fd传给select或者poll系统调用，阻塞在select，这样select/poll就可以检测是否有fd处于就绪状态。 当数据准备就绪时，为该进程生成一个SIGIO信号，通知进程来读取数据。 以socket接口为例：进程空间调用recvFrom，其系统调用知道数据包到达且被写到应用进程缓冲区或者发生错误时返回，在此期间一直在等待。进程从开始调用recvFrom开始一直被占用所以叫阻塞模型。 recvFrom从应用到内核，如果缓冲区没有数据的话，就会返回一个EWOULDBLOCK错误，一般都对非阻塞IO模型进行轮询检查内核是不是有数据到来。 顺序扫描，支持的fd有限；还提供epoll系统调用，基于事件驱动方式代替顺序扫描，性能更好","tags":[]},{"title":"数据存储/缓存/redis数据结构整理","date":"2019-01-01T13:18:50.746Z","path":"2019/01/01/数据存储/缓存/redis数据结构整理/","text":"Redis 数据结构 扁平化的特点，不存在数据库中列表查询一类的操作 数据结构 Value对象的通用数据结构 1234567typedef struct redisObject &#123; unsigned type:4; unsigned encoding:4; unsigned lru:REDIS_LRU_BITS; int refcount; void *ptr;&#125;robj; type：指String、List等结构化类型 encoding：encoding指的是这些接口规划类型具体的实现（承载）方式，同一个类型可以有多个实现方式，例如String可以用int来承载，也可以用封装的cha[]来承载，List可以用ziplist或者链表来承载 lru：表示本对象的空转时长，用于有限内存下长久不访问的对象的清理 refcount：应用计数用于对象的垃圾回收 ptr：指向的是以encoding方式实现这个对象的实际承载者的地址，例如String对象对应的sds地址 String Redis中的String可以表示字符串、整数、浮点数三种类型，由Redis完成相互间的自动转型 基本操作： Redis-String操作 List 基本操作： RPUSH/LPUSH：RPUSH将元素添加在列表尾，L则是将元素添加在列表头部 RPOP/LPOP：取出给定key的列表的尾部或头部的元素并删除元素 LINDEX：去除给定的key对应列表索引的某个元素 LRANGE：取出给定key的索引范围内的元素，例如LRANGE key1 0, 3即取出前四个元素 LTRIM：将给定key的列表索引范围的元素去除 BLPOP/BRPOP：BRPOP key1 key2 60，60秒内，key1非空则从key1对应的列表中pop最右元素，否则从key2中pop最右元素；如果60秒内两个列表始终为空，则超时返回 BLPOPPUSH/BRPOPPUSH：BRPOPPUSH key1 key2 60即60秒内如果key1对应的列表非空，则把key1列表的最右元素pop，并且放到key2最后 Map： 基本操作： HGET：返回给定key， field的值 HSET：设置给定key，field的值 HMGET：返回给定key，field1、field2…的值 HMSET：设置给定key，多个field的值 HGETALL：获取给定key的所有field和value HDEL：删除给定key， field元素 HKEYS：获取给定的key的所有field名字 HVALS：获取给定key的所有value HLEN：获取给定的key的字段数量 HINCRBY：HINCRBY key field increment，给定key的field元素value自增整数 HINCRBYFLOAT：HINCRBYFLOAT key field increment ，给定key的field元素value自增浮点数 HEXISTS：检查指定key field是否存在 HSETNX：HSETNX key field value只有当field字段不存在时，设置该元素 Set： 基本操作： SADD/SREM/SISMEMBER：实现向SET中增加、删除元素，以及检查元素是否存在 SCAD/SMEMBERS/SRANDMEMBER：实现统计元素个数、列出所有元素、随机获取元素的操作 Sorted-Set： ZRANK：确定某个KEY值在本sorted-set内按照顺序排在第几位 ZRANGE：例如ZRANGE key start stop，获取sorted-set中排名为start和stop间的数据 ZRANGESCOPE：ZRANGEBYSCOPE key min max获取sorted-set中scope介于min和max之间的数据 ZSCOPE：确定某个key值在本sorted-set内对应的value ZINCRBY","tags":[]},{"title":"net-work/okhttp/OKHTTP学习","date":"2019-01-01T13:18:05.057Z","path":"2019/01/01/net-work/okhttp/OKHTTP学习/","text":"来自王老板的分享 OKHTTP系统架构模型 OKHTTP系统架构模型（王召） 线程池 线程模型（王召） 连接池（ConnectionPool） 连接池模型（王召） 思考 概述 如何创建 如何管理 如何回收 要解决的问题 连接池上限 什么时候释放哪些连接 有链接超时的情况 学习 协议 HTTP 1.0 HTTP 1.1 HTTP 2 SPDY 3.1 QUIK (Quick UDP Internet Connection) 创建 put新连接 先检查空闲连接，将其清理 放入新的连接 12345678void put(RealConnection connection) &#123; assert (Thread.holdsLock(this)); if (!cleanupRunning) &#123; cleanupRunning = true; executor.execute(cleanupRunnable); &#125; connections.add(connection);&#125; 123456789101112131415161718192021private final Runnable cleanupRunnable = new Runnable() &#123; @Override public void run() &#123; //循环执行清理操作 while (true) &#123; //返回清理执行等待的纳秒数 long waitNanos = cleanup(System.nanoTime()); if (waitNanos == -1) return; if (waitNanos &gt; 0) &#123; long waitMillis = waitNanos / 1000000L; waitNanos -= (waitMillis * 1000000L); synchronized (ConnectionPool.this) &#123; try &#123; ConnectionPool.this.wait(waitMillis, (int) waitNanos); &#125; catch (InterruptedException ignored) &#123; &#125; &#125; &#125; &#125; &#125;&#125;; 清理 如何找到闲置的连接 通过pruneAndGetAllocationCount(connection, now)判断当前连接是不是在用 当idleDurationNs纳秒数超过keepAliveDurationNs或者idleConnectionCount超过maxIdleConnections时，直接将当前连接移除 上面情况不满足时，当idleConnectionCount &gt; 0返回允许等待的时间差值 当inUseConnectionCount &gt; 0返回keepAlive的最大时间 当前无连接不需要清理 如何判断连接是否在用 主要检查Reference的StreamAllocation是否为空，为空则说明有连接泄漏，程序有异常，不为空则返回Reference的列表size","tags":[]},{"title":"数据存储/缓存/网站架构中的缓存结构整理","date":"2019-01-01T13:16:21.593Z","path":"2019/01/01/数据存储/缓存/网站架构中的缓存结构整理/","text":"网站架构中的缓存 image 网站架构缓存如图可分为： 客户端缓存 页面缓存 将之前渲染的页面保存为文件，当用户再次访问时可以避开网络连接，从而减少负载，提升性能和用户体验。 浏览器缓存 HTTP1.0：与服务器约定规则进行，在服务器侧设置Expires的HTTP头来告诉客户端在重新请求文件之前多久是安全的，可以使用if-midified-since的条件请求来清空缓存，比较文件最初的下载时间和最后的更新时间，如果文件没有改变可以用304-Not Modified应答客户端 HTTP1.1：缓存系统被形式化引入了e-Tag标签，e-Tag标签是文件或者对象的唯一标识，当询问服务器某一个资源的e-Tag标签是否有效，有效会生成304-Not Modified；否则返回200提供正确的文件 e-Tag流程图 Cache-Control/Expires和Last-Modified/ETag一期使用时，Cache-Control/Expires的优先级要高于后者。即当本地缓存根据Cache-Control/Expires判断还在有效期内时，就不会在去服务器询问修改时间和实体标识了。 在html页面可以添加（并不是所有浏览器都支持） &lt;META HTTP-EQUIV=\"Pragma\" CONTENT=\"no-cache\"&gt; APP上缓存 数据库缓存：把文件的相关信息（URL、路径、下载时间、过期时间）等放到数据库中，下次查询先从数据库中查询做对比 文件缓存：使用文件操作的API获取文件的最后修改时间。图片和其他配置类文件的缓存时间不一样。图片的缓存时间可能可以持续到下次清空缓存，但是配置文件可能会被更新；在不同网络环境下缓存的更新时间也可以不一样 网络中缓存 WEB代理缓存 正向代理 反向代理：客户端向代理服务发送请求，反向代理自己判断向何处发送请求，然后将从源服务获取到的内容返回给客户端 透明代理：客户端根本不需要知道有代理服务器的存在，又代理服务器改变客户端的请求报文，并传送真实的IP地址 匿名代理：加密的透明代理 边缘缓存 反向代理服务和用户来源于同一个网络，用户访问反向代理服务就会得到较高质量的响应，这种反向代理缓存叫做边缘缓存 边缘缓存的商业化服务-CDN CDN缓存也是通过HTTP响应头的Cache-Control:max-age字段来设置CDN边缘节点的数据缓存时间，当客户端向CDN节点请求数据时，CDN回先判断缓存时间是否过期，没有过期则将缓存返回给客户端，过期则像服务器请求数据并更新本地缓存。 服务端缓存 数据库缓存 query cache：作用于MySQL实例，主要针对于select语句，MySQL将接收到的select语句以字符串进行hash，然后在query cache中进行查找，如果有就返回query cache的内容。 query cache：需要query_cache_size和query_cache_type；前者设置缓存大小，后者表示在那种场景下使用（0-OFF，1-ON，2-DEMAND） Qcache inserts表示多少次未命中然后插入；Qcache lowmem prunes值大则说明缓存不够；Qcache hits值非常大，则说明使用缓存较频繁；Qcache free blocks表明缓存区的碎片，如果较多需要清理. InnoDB缓存：innodb_buffer_pool_size设置缓存InnoDB索引及数据块的内存区域大小；table_cache设置高速缓存的数量。 平台级缓存 EhCache： 轻量快速：线程机制为大型高并发系统设计 良性伸缩：数据可以伸缩到数G字节，节点可以到数百个 简洁灵活：运行时缓存设置，存活时间、空闲时间、内存和缓存存放的最大数目可以在运行时修改 标准支持： 强扩展性：节点发现，冗余器和监听器可插件化 数据持久：缓存的数据在机器重启后可以在磁盘上获取 缓存监听：提供对缓存事件之后的处理机制 分布式缓存：支持高性能的分布式缓存，兼具灵活性和扩展性 应用级缓存 Redis： 应用缓存技术： 缓存命中：缓存中有这一对象，则使用缓存的数据 没有命中：cache miss ，缓存中还有空间的时候，没有命中的对象就会被放入缓存中 存储成本：没有命中缓存，需要从数据库中取出数据放入缓存，这个过程消耗的时间和空间叫做存储成本 缓存失效：存储在缓存中的数据要被更新时，则原数据就失效了 替代策略：当缓存已经满了，有新的数据来，需要从缓存中去除一条旧的数据放入新的数据，如何去除如何插入新的需要有替代策略 替代策略： Least-Recently-Used（LRU）：替换掉最近最少请求的对象 Least-Frequently-Used（LFU）：替换掉访问次数最少的对象 Least-Recently-Used 2（LRU2）：把最近访问两次的对象放入缓存，会把最近两次使用最少的缓存对象去除，需要跟踪对象两次，访问负载会增加 Two Queues（2Q）：把被访问的数据放到KRU的缓存中，如果这个对象在被访问一次，就把这个对象放到第二个、更大的LRU缓存中，使用多级缓存 SIZE：替换占用空间最大的对象，这样会导致某些小对象可能一直存在与缓存中 LRU-Threshold：不缓存超过某一大小的对象，其他与LRU相同 Log(size)+LRU：替换最大的对象，size相同时按照LRU处理 Hyper-G：LFU的改进，同时考虑上一次访问时间和大小 Pitkow/Recker：替换最近最少使用的对象，除非所有对象都是今天使用的；如果都是今天则替换掉最大的对象 Lowest-Latency-First：替换下载时间最少的文件 Hybrid Hybrid：减少平均延迟，对缓存中的每个文档计算一个保留效用，保留效用最小的对象会被替换掉 Lowest Relative Value（LRV）：计算保留效用，替换保留效用最低的对象 Adaptive Replacement Cache（APC）：介于LRU和LFU中间，由两个LRU组成，第一个LRU包含最近只被使用过一次的，第二个包含最近被使用过两次的，以此得到新的和常用的对象；APC可以自我调节，访问负载较小 Most Recently Used（MRU）：MRU与LRU是相对的，移除最近被最多使用的对象；（减少因为计算寻找最少最近的耗时） First in First out（FIFO）：通过队列跟踪缓存对象，最先进入的对象最先被踢走 Random Cache：随意替换，效果比FIFO好，有些情况比LRU好 云缓存服务：动态扩容、数据多备、自动容灾、成本较低","tags":[]},{"title":"java/jvm/JVM内存回收","date":"2019-01-01T13:02:10.914Z","path":"2019/01/01/java/jvm/JVM内存回收/","text":"JVM 内存回收 蛮荒之地，笔者正在火速开垦……","tags":[]},{"title":"net-work/http/HTTP2.0的价值在哪里","date":"2019-01-01T13:02:10.914Z","path":"2019/01/01/net-work/http/HTTP2.0的价值在哪里/","text":"HTTP 2.0 的价值在哪里 蛮荒之地，笔者正在火速开垦……","tags":[]},{"title":"net-work/okhttp/OkHttpClient3线程模型","date":"2019-01-01T13:02:10.914Z","path":"2019/01/01/net-work/okhttp/OkHttpClient3线程模型/","text":"OkHttpClient3 线程模型 蛮荒之地，笔者正在火速开垦……","tags":[]}]